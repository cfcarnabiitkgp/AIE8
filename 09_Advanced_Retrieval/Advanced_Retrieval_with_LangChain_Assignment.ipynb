{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ðŸ¤ Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ðŸ¤ Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable LangSmith (same as test notebook)\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Use Case Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/Projects_with_Domains.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Project Title\",\n",
        "      \"Project Domain\",\n",
        "      \"Secondary Domain\",\n",
        "      \"Description\",\n",
        "      \"Judge Comments\",\n",
        "      \"Score\",\n",
        "      \"Project Name\",\n",
        "      \"Judge Score\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "synthetic_usecase_data = loader.load()\n",
        "\n",
        "for doc in synthetic_usecase_data:\n",
        "    doc.page_content = doc.metadata[\"Description\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 50\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of documents: {len(synthetic_usecase_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"Synthetic_Usecases\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    synthetic_usecase_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecases\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Healthcare / MedTech,\" which is mentioned multiple times in the dataset.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security. Specifically, one project titled \"LatticeFlow\" is described as an AI-powered platform optimizing logistics routes for sustainability, and it is associated with the Healthcare / MedTech domain and the secondary domain of Security.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'There are 0 projects that directly belong to the Security domain in the provided data.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Can you count how many projects belonged to Security domain?\"})[\"response\"].content\n",
        "\n",
        "# this is not good, it is not able to count the number of projects belonged to Security domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had various comments about the fintech projects. For example, they described the project \"PlanPilot 35\" as \"A clever solution with measurable environmental benefit,\" and \"PixelSense\" as having a \"Comprehensive and technically mature approach.\" Overall, the judges viewed the projects positively, highlighting the strength of ideas, technical execution, and real-world impact in some cases.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(synthetic_usecase_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain is not explicitly stated, but among the sample projects listed, the domains include Productivity Assistants, Legal / Compliance, Data / Analytics, and Healthcare / MedTech. Since only a small sample is shown, I cannot definitively determine which domain is most common overall. If you have access to the full dataset, it would be best to analyze the frequency of each domain there.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = bm25_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Eâ€‘commerce / Marketplaces,\" as it is mentioned as a secondary domain for some projects. However, since only a few sample entries are provided and \"Project Domain\" entries include \"Productivity Assistants,\" \"Legal / Compliance,\" \"Data / Analytics,\" and \"Healthcare / MedTech,\" and no clear count for each, the most frequent project domain cannot be determined definitively from this snippet alone.\\n\\nIf considering the sample data, \"Eâ€‘commerce / Marketplaces\" appears more than once as a secondary domain, but for the primary domains, there isn\\'t enough information.\\n\\nTherefore, I do not have enough data to conclusively identify the most common project domain.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided information, there are no specific use cases related to security mentioned in the context.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges commented that the fintech project, PulseAI 50, was technically ambitious and well-executed.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### âœ… Answer: BM25 excels at exact or near-exact keyword mathcing because it is based on the bag-of-words representation. It works best when:\n",
        "\n",
        "- User query contains exact phrases that appear in the document\n",
        "- Searching for technical terms, proper nouns, or domain-specific vocab\n",
        "- Query consists of important keywords that appear in the rerieved documents\n",
        "\n",
        "Lets check a scenario where BM25 retriever generates a \"better\" (vibe) response than embedding-based naive retriever. Here I use a query that is not super descriptive (or even gramatically correct) but rather uses a selection of phrases from project description of the project named \"TaskFlow 13\". You will see the response using BM25 is more descriptive and satisfying. ðŸ‘‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Retrieval Response: \n",
            " The project described as a \"dynamic model pruning toolkit\" reduces compute costs by 40%.\n",
            "\n",
            "\n",
            " BM25 Retrieval Response: \n",
            " The project that focuses on reducing compute costs by 40% is \"TaskFlow 13,\" which is in the Legal / Compliance domain with a secondary domain in Eâ€‘commerce / Marketplaces.\n"
          ]
        }
      ],
      "source": [
        "# query to test BM25 vs naive retriever\n",
        "query = \"project compute costs by 40%\"\n",
        "\n",
        "response_naive = naive_retrieval_chain.invoke({\"question\" : query})[\"response\"].content\n",
        "response_bm25 = bm25_retrieval_chain.invoke({\"question\" : query})[\"response\"].content\n",
        "\n",
        "print(f\"Naive Retrieval Response: \\n {response_naive}\")\n",
        "print(f\"\\n\\n BM25 Retrieval Response: \\n {response_bm25}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\", top_n=7)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data is \"Healthcare / MedTech,\" which appears multiple times among the listed projects.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. For example, one project describes \"a federated learning toolkit improving privacy in healthcare applications,\" which is directly related to security and privacy concerns.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges\\' comments on the fintech projects were generally positive. For example, one project received praise for its \"excellent code quality and use of open-source libraries,\" and another was noted for being a \"clever solution with measurable environmental benefit.\" Overall, the feedback highlighted the technical quality, promising ideas, and real-world impact of the projects related to fintech.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ") # by default it will generate 3 queries ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data is \"Healthcare / MedTech,\" appearing multiple times among the projects.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. Specifically, the project \"LearnWise 39\" within the \"Data / Analytics\" domain focuses on an AI model compression suite enabling on-device reasoning for IoT sensors, which can have security implications. Additionally, \"SecureNest 28\" in the \"Legal / Compliance\" domain is a hardware-aware model quantization benchmark suite, indicating a focus on security-related benchmarking.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had various opinions about the fintech projects. For example, they described some projects as having \"excellent code quality and use of open-source libraries,\" indicating a positive view of technical execution. Others were noted for being \"conceptually strong,\" \"technically ambitious and well-executed,\" or having \"impressive real-world impact.\" Some projects received high scores, such as 96 and 94, with comments highlighting their originality and environmental benefits. However, there were also mentions of minor issues or the need for more benchmarking results, suggesting that while the projects were viewed favorably overall, some improvements or additional validation were recommended.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### âœ… Answer: \n",
        "\n",
        "The recall metric answers the question \" of all relevant documents that exist, what percentage of it are successfully retrieved?\" A higher recall means we are finding most of the relevant context, reducing chances of missing important information. Multi query retrieval (essentually generating multiple variants of an user query) improves recall in the following ways:\n",
        "\n",
        "- Users often address same information in different ways (e.g. different personas, tones, phrasing, synonyms etc.) so multi-variants can address query ambiguity\n",
        "- It expands semantic coverage and increases the chances of finding documents that are relevant but expressed differently\n",
        "- Even best embedding models can miss relevant documents due to vocab mismatches, sentence structures, tonality and contextual variations so multi-queries act as a safety net\n",
        "- It increases document pool diversity and union of all retrieved docs provide a more comprehensive context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = synthetic_usecase_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)  # changing chunk size from 750 to 500 seemed to improve response quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "#store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=InMemoryStore(),\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"Healthcare / MedTech,\" as it is mentioned twice among the sample entries.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there do not appear to be any use cases specifically about security. The projects mentioned focus mainly on federated learning to improve privacy in healthcare applications, but there is no explicit mention of security-related use cases.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges\\' comments about the fintech projects included describing one project as \"a clever solution with measurable environmental benefit,\" and another as \"technically ambitious and well-executed.\" These comments suggest that the judges found the fintech-related project to be both innovative and effectively implemented.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data is Healthcare / MedTech, as it appears multiple times among the listed projects.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. Specifically, the project titled \"SecureNest 28\" falls under the legal and compliance domain and involves a hardware-aware model quantization benchmark suite, which relates to security and compliance measures.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges\\' comments on the fintech projects were generally positive but varied in specifics. For example:\\n\\n- The project \"Pathfinder 27\" received praise for \"excellent code quality and use of open-source libraries.\"\\n- \"SecureNest 28\" was recognized as \"conceptually strong but results need more benchmarking.\"\\n- \"Pathfinder 24\" was noted for being \"well-structured and scalable\" with good potential for commercialization.\\n\\nOverall, judges highlighted strengths such as code quality, scalability, and potential for commercialization, while also noting areas like benchmarking and UI design that could be improved.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(synthetic_usecase_data[:20])\n",
        "len(semantic_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecase_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Legal / Compliance,\" which is mentioned more than once compared to other domains.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security in the provided data. Specifically:\\n\\n1. \"MediMind 17\" with the project name \"BioForge,\" is described as a medical imaging solution improving early diagnosis through vision transformers, and it is categorized under the Security domain.\\n2. \"InsightAI 1\" with the project name \"Project Aurora,\" involves a low-latency inference system for multimodal agents in autonomous systems, also categorized under the Security domain.\\n3. \"SecureNest 12\" with the project name \"Neural Canvas,\" features a low-latency inference system for multimodal agents in autonomous systems within the Security domain.\\n\\nThese projects are focused on security-related applications, particularly involving inference systems and autonomous systems.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had quite positive comments about the fintech projects. For example, they described the projects as \"Comprehensive and technically mature\" and \"Technically ambitious and well-executed.\" In addition, some projects were noted for being \"Well-structured and scalable\" with \"good potential for commercialization,\" and others as \"A forward-looking idea with solid supporting data.\" Overall, the judges appreciated the technical quality, ambition, and potential impact of the fintech-related projects.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### âœ… Answer\n",
        "\n",
        "Note that semantic chunking works by comparing adjacent sentence embeddings, creating chunks where topic shifts occur based on semantic meaning rather than fixed character counts.\n",
        "\n",
        "With FAQ-style content, semantic chunking may tend to:\n",
        "\n",
        "- Over-merge unrelated items - questions starting with \"How do I...\", \"What is...\", \"Can I...\" show high similarity scores despite different topics\n",
        "- Struggle with nuance - short sentences lack semantic richness, making distinctions harder\n",
        "- Create inconsistent chunks - similar phrasing may override topical differences\n",
        "\n",
        "In such a case I cna think of three approaches:\n",
        "\n",
        "1. Lower the similarity threshold, e.g., \n",
        "    ```python \n",
        "\n",
        "        chunker = SemanticChunker(\n",
        "            embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "            breakpoint_threshold_amount=70  # More aggressive splitting\n",
        "        )\n",
        "    ```\n",
        "2. Use `breakpoint_threshold_type=\"gradient\"` - the [LangChain documentation](https://python.langchain.com/docs/how_to/semantic-chunker/#gradient) mentions that this threshold method chunks are highly correlated with each other. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data. \n",
        "\n",
        "3.Add metadata (question type, category) before chunking - this is a bit Meta!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ðŸ¤ Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Processing + Document Generation Using PDF + CSV Data\n",
        "\n",
        "There are some issues with the current `Projects_with_Domain.csv` dataset. For example,\n",
        "\n",
        "- Same project name appears for diffeent project titles (e.g., `Neural Canvas` name maps to both `CreateFlow 33` and `TrendLens 32`)\n",
        "\n",
        "- Same descriptions appear for different projects (e.g.,  \"A low-latency inference system...\" appears multiple times)\n",
        "\n",
        "- Same project name have different scores (e.g., \"BioForge\" appears with scores 9.7, 7.5, 7.0, 7.8)\n",
        "\n",
        "- Domain inconsistencies: Same project titles in different domains\n",
        "\n",
        "\n",
        "It is interesting to note that project titles are indeed unique but project description and names are not, which is indicative of spurious data. However, we really care about the project descriptions, primary and secondary domains, and may add other secondary metadata (such as judge comments) directly into the Document page content.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of records after cleaning: 20\n",
            "Removed 30 duplicate descriptions\n",
            "Cleaned dataset saved to ./data/cleaned_Projects_with_Domains.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from langchain.schema import Document \n",
        "\n",
        "def create_cleaned_dataset_from_existing_docs(docs: list[Document], save_data: bool = True) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Hybrid approach: Clean existing Documents by removing duplicates while \n",
        "    preserving ALL original metadata and augmenting page_content.\n",
        "    \n",
        "    Args:\n",
        "        docs: List of Documents with metadata (e.g., from CSVLoader)\n",
        "    \n",
        "    Returns:\n",
        "        List of cleaned Documents with augmented page_content and preserved metadata\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert docs to dataframe for deduplication\n",
        "    records = []\n",
        "    for i, doc in enumerate(docs):\n",
        "        record = doc.metadata.copy()\n",
        "        record['Description'] = doc.metadata.get('Description', doc.page_content)\n",
        "        record['_original_idx'] = i\n",
        "        record['_page_content'] = doc.page_content  # Preserve original\n",
        "        records.append(record)\n",
        "    \n",
        "    df = pd.DataFrame(records)\n",
        "    \n",
        "    # Clean: keep only unique descriptions, preferring higher judge scores\n",
        "    df_cleaned = (\n",
        "        df.sort_values(by=\"Judge Score\", ascending=False)\n",
        "        .drop_duplicates(subset=[\"Description\"], keep=\"first\")\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    \n",
        "    print(f\"Length of records after cleaning: {len(df_cleaned)}\")\n",
        "    print(f\"Removed {len(df) - len(df_cleaned)} duplicate descriptions\")\n",
        "    \n",
        "    # Create cleaned documents with augmented page_content\n",
        "    cleaned_docs = []\n",
        "    for _, row in df_cleaned.iterrows():\n",
        "        # Augment page_content for better semantic chunking (avoiding tiny chunks)\n",
        "        page_content = f\"\"\"\n",
        "Description: {row['Description']}\n",
        "\n",
        "Primary Domain: {row['Project Domain']}\n",
        "\n",
        "Secondary Domain: {row['Secondary Domain']}\n",
        "\n",
        "Expert comments: {row['Judge Comments']}\n",
        "        \"\"\".strip()\n",
        "        \n",
        "        # Preserve ALL original metadata\n",
        "        metadata = row.to_dict()\n",
        "        \n",
        "        # Add processing metadata for tracking\n",
        "        metadata.update({\n",
        "            'source': metadata.get('source', 'csv_cleaned'),\n",
        "            'doc_type': 'project',\n",
        "            'augmented_content': True,  # Flag that content was enhanced\n",
        "            'quality_tier': 'high' if float(row.get('Judge Score', 0)) > 8.0 else 'medium',\n",
        "        })\n",
        "        \n",
        "        doc = Document(\n",
        "            page_content=page_content,\n",
        "            metadata=metadata\n",
        "        )\n",
        "        cleaned_docs.append(doc)\n",
        "    \n",
        "    # Optional: Save cleaned CSV for reference\n",
        "    if save_data:\n",
        "        cleaned_csv_path = \"./data/cleaned_Projects_with_Domains.csv\"\n",
        "        df_to_save = df_cleaned.drop(columns=['_original_idx', '_page_content', 'source', 'row'], errors='ignore')\n",
        "        df_to_save.to_csv(cleaned_csv_path, index=False)\n",
        "        print(f\"Cleaned dataset saved to {cleaned_csv_path}\")\n",
        "    \n",
        "    return cleaned_docs\n",
        "\n",
        "# Usage:\n",
        "csv_docs = create_cleaned_dataset_from_existing_docs(synthetic_usecase_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents loaded from PDF files: 64\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "\n",
        "# load pdf data \n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "pdf_docs = loader.load()\n",
        "\n",
        "print(f\"Number of documents loaded from PDF files: {len(pdf_docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of documents (from PDF and CSV sources): 84\n"
          ]
        }
      ],
      "source": [
        "# combine csv and pdf documents (using all of the data available)\n",
        "final_evaluation_docs = pdf_docs + csv_docs\n",
        "print(f\"Total number of documents (from PDF and CSV sources): {len(final_evaluation_docs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Redefine Retrievers (using Recursive Text Split Chunking) with Combined Dataset \n",
        "\n",
        "The original retrievers were built using only the 50 CSV documents, but our test set was generated from the combined 84 documents (PDF + CSV). We need to rebuild all retrievers using the `final_evaluation_docs` to ensure they search the same corpus that generated our test questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Rebuilding main vectorstore with combined dataset...\n",
            "âœ… Vectorstore created with 84 documents\n",
            "âœ… Naive retriever updated\n",
            "âœ… BM25 retriever updated\n",
            "âœ… Contextual compression retriever updated\n",
            "âœ… Multi-query retriever updated\n",
            "âœ… Parent document retriever updated\n",
            "âœ… Ensemble retriever updated\n",
            "\n",
            "ðŸŽ‰ All retrievers rebuilt with 84 documents!\n"
          ]
        }
      ],
      "source": [
        "# Rebuild main vectorstore with combined data\n",
        "print(\"ðŸ”„ Rebuilding main vectorstore with combined dataset...\")\n",
        "vectorstore_updated = Qdrant.from_documents(\n",
        "    final_evaluation_docs,  # âœ… Use combined PDF + CSV docs\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecases_Updated\"\n",
        ")\n",
        "print(f\"âœ… Vectorstore created with {len(final_evaluation_docs)} documents\")\n",
        "\n",
        "# Rebuild naive retriever\n",
        "naive_retriever_updated = vectorstore_updated.as_retriever(search_kwargs={\"k\": 10})\n",
        "print(\"âœ… Naive retriever updated\")\n",
        "\n",
        "# Rebuild BM25 retriever\n",
        "bm25_retriever_updated = BM25Retriever.from_documents(final_evaluation_docs)\n",
        "bm25_retriever_updated.k = 10  # Match the k value of other retrievers\n",
        "print(\"âœ… BM25 retriever updated\")\n",
        "\n",
        "# Rebuild contextual compression retriever\n",
        "compressor_updated = CohereRerank(model=\"rerank-v3.5\", top_n=7)\n",
        "compression_retriever_updated = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor_updated, \n",
        "    base_retriever=naive_retriever_updated\n",
        ")\n",
        "print(\"âœ… Contextual compression retriever updated\")\n",
        "\n",
        "# Rebuild multi-query retriever\n",
        "multi_query_retriever_updated = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever_updated, \n",
        "    llm=chat_model\n",
        ")\n",
        "print(\"âœ… Multi-query retriever updated\")\n",
        "\n",
        "# Rebuild parent document retriever\n",
        "# Create new QDrant client and collection\n",
        "client_updated = QdrantClient(location=\":memory:\")\n",
        "client_updated.create_collection(\n",
        "    collection_name=\"full_documents_updated\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore_updated = QdrantVectorStore(\n",
        "    collection_name=\"full_documents_updated\", \n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
        "    client=client_updated\n",
        ")\n",
        "\n",
        "# Create parent document retriever\n",
        "parent_document_retriever_updated = ParentDocumentRetriever(\n",
        "    vectorstore=parent_document_vectorstore_updated,\n",
        "    docstore=InMemoryStore(),\n",
        "    child_splitter=child_splitter,  # Reuse the same splitter\n",
        ")\n",
        "\n",
        "# Add the combined documents\n",
        "parent_document_retriever_updated.add_documents(final_evaluation_docs, ids=None)\n",
        "print(\"âœ… Parent document retriever updated\")\n",
        "\n",
        "\n",
        "# Rebuild ensemble retriever\n",
        "retriever_list_updated = [\n",
        "    bm25_retriever_updated, \n",
        "    naive_retriever_updated, \n",
        "    parent_document_retriever_updated, \n",
        "    compression_retriever_updated, \n",
        "    multi_query_retriever_updated\n",
        "]\n",
        "equal_weighting = [1/len(retriever_list_updated)] * len(retriever_list_updated)\n",
        "\n",
        "ensemble_retriever_updated = EnsembleRetriever(\n",
        "    retrievers=retriever_list_updated, \n",
        "    weights=equal_weighting\n",
        ")\n",
        "print(\"âœ… Ensemble retriever updated\")\n",
        "print(f\"\\nðŸŽ‰ All retrievers rebuilt with {len(final_evaluation_docs)} documents!\")\n",
        "\n",
        "\n",
        "# Rebuild ensemble retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Rebuilding RAG chains...\n",
            "âœ… All RAG chains rebuilt!\n"
          ]
        }
      ],
      "source": [
        "# Rebuild the RAG chains with updated retrievers\n",
        "print(\"ðŸ”„ Rebuilding RAG chains...\")\n",
        "\n",
        "# Naive retrieval chain\n",
        "naive_retrieval_chain_updated = (\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever_updated, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# BM25 retrieval chain\n",
        "bm25_retrieval_chain_updated = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever_updated, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Contextual compression chain\n",
        "contextual_compression_retrieval_chain_updated = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever_updated, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Multi-query chain\n",
        "multi_query_retrieval_chain_updated = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever_updated, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Parent document chain\n",
        "parent_document_retrieval_chain_updated = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever_updated, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Ensemble chain\n",
        "ensemble_retrieval_chain_updated = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever_updated, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "print(\"âœ… All RAG chains rebuilt!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use RAGAS for Synthetic Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(\n",
        "    ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "    )\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(\n",
        "    OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57ebfc45c1c449298b1aac22988d4e8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a593faf5ff83493986c9296d6e87b9e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/84 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "030710f4993e4a93a35fd6f6ceccccf8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node '3f83d3'. Skipping!\n",
            "Property 'summary' already exists in node '7f3a37'. Skipping!\n",
            "Property 'summary' already exists in node 'c490d8'. Skipping!\n",
            "Property 'summary' already exists in node '042eac'. Skipping!\n",
            "Property 'summary' already exists in node '69261f'. Skipping!\n",
            "Property 'summary' already exists in node 'dead5c'. Skipping!\n",
            "Property 'summary' already exists in node '56e422'. Skipping!\n",
            "Property 'summary' already exists in node '7ccb8c'. Skipping!\n",
            "Property 'summary' already exists in node 'c33412'. Skipping!\n",
            "Property 'summary' already exists in node '245656'. Skipping!\n",
            "Property 'summary' already exists in node '1fce7e'. Skipping!\n",
            "Property 'summary' already exists in node '187874'. Skipping!\n",
            "Property 'summary' already exists in node '5517c6'. Skipping!\n",
            "Property 'summary' already exists in node '2bf359'. Skipping!\n",
            "Property 'summary' already exists in node '74e74a'. Skipping!\n",
            "Property 'summary' already exists in node '9f8df6'. Skipping!\n",
            "Property 'summary' already exists in node '9e10fb'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cf6eb2bf98f485680e357fa392f5cd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb532931c0b949d8bf932ef7988f1d87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node '042eac'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '245656'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '9e10fb'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '56e422'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '69261f'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '7f3a37'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'dead5c'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '7ccb8c'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '5517c6'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'c33412'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'c490d8'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '3f83d3'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '1fce7e'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '187874'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '2bf359'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '9f8df6'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '74e74a'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cec677a723d041f3a6f1d1a27cd4c2da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e25517c099d4699a4934ab6c2c96ecc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a26aeae4cf91494db649ea0105a8e6b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f4c9d55ca5f4cf890dc7ff8750778ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "testset = generator.generate_with_langchain_docs(documents=final_evaluation_docs, testset_size=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the expected developments for ChatGPT...</td>\n",
              "      <td>[Introduction ChatGPT launched in November 202...</td>\n",
              "      <td>The provided context does not specify any deta...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OpenAI how does it help work?</td>\n",
              "      <td>[Table 1: ChatGPT daily message counts (millio...</td>\n",
              "      <td>The context explains that OpenAI's ChatGPT is ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the usage of ChatGPT differ among use...</td>\n",
              "      <td>[Variation by Occupation Figure 23 presents va...</td>\n",
              "      <td>Users in nonprofessional occupations, includin...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What does the paper reveal about ChatGPT usage...</td>\n",
              "      <td>[Conclusion This paper studies the rapid growt...</td>\n",
              "      <td>The paper indicates that in the US, ChatGPT us...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the message volume comparison between...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nMonth Non-Work (M) (%) Work (M) (%...</td>\n",
              "      <td>In June 2024, non-work messages accounted for ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How does the rapid growth and widespread adopt...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nConclusion This paper studies the ...</td>\n",
              "      <td>The first context segment highlights that by J...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Based on the analysis of ChatGPT's usage patte...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nConclusion This paper studies the ...</td>\n",
              "      <td>The context indicates that ChatGPT has experie...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Based on the message volume comparison between...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nMonth Non-Work (M) (%) Work (M) (%...</td>\n",
              "      <td>In June 2024, non-work messages accounted for ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the increase in total message volume ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nMonth Non-Work (M) (%) Work (M) (%...</td>\n",
              "      <td>The data shows that total messages increased f...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How did ChatGPT's usage patterns and message v...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nTable 1: ChatGPT daily message cou...</td>\n",
              "      <td>Between June 2024 and June 2025, ChatGPT exper...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>whats the diff between june 2024 and june 2025...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nTable 1: ChatGPT daily message cou...</td>\n",
              "      <td>The context indicates that by June 2024, ChatG...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>WHy is the US so important in the context of C...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nTable 1: ChatGPT daily message cou...</td>\n",
              "      <td>The context highlights that the US is signific...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>How does the growth of ChatGPT usage in the US...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nTable 1: ChatGPT daily message cou...</td>\n",
              "      <td>The context indicates that in the US, ChatGPT ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>wha november 2022 did ChatGPT lauched and how ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
              "      <td>ChatGPT was launched in November 2022. By July...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>How does the rapid growth of ChatGPT by July 2...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
              "      <td>By July 2025, ChatGPT had experienced unpreced...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What are the expected developments for ChatGPT...   \n",
              "1                       OpenAI how does it help work?   \n",
              "2   How does the usage of ChatGPT differ among use...   \n",
              "3   What does the paper reveal about ChatGPT usage...   \n",
              "4   How does the message volume comparison between...   \n",
              "5   How does the rapid growth and widespread adopt...   \n",
              "6   Based on the analysis of ChatGPT's usage patte...   \n",
              "7   Based on the message volume comparison between...   \n",
              "8   How does the increase in total message volume ...   \n",
              "9   How did ChatGPT's usage patterns and message v...   \n",
              "10  whats the diff between june 2024 and june 2025...   \n",
              "11  WHy is the US so important in the context of C...   \n",
              "12  How does the growth of ChatGPT usage in the US...   \n",
              "13  wha november 2022 did ChatGPT lauched and how ...   \n",
              "14  How does the rapid growth of ChatGPT by July 2...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Introduction ChatGPT launched in November 202...   \n",
              "1   [Table 1: ChatGPT daily message counts (millio...   \n",
              "2   [Variation by Occupation Figure 23 presents va...   \n",
              "3   [Conclusion This paper studies the rapid growt...   \n",
              "4   [<1-hop>\\n\\nMonth Non-Work (M) (%) Work (M) (%...   \n",
              "5   [<1-hop>\\n\\nConclusion This paper studies the ...   \n",
              "6   [<1-hop>\\n\\nConclusion This paper studies the ...   \n",
              "7   [<1-hop>\\n\\nMonth Non-Work (M) (%) Work (M) (%...   \n",
              "8   [<1-hop>\\n\\nMonth Non-Work (M) (%) Work (M) (%...   \n",
              "9   [<1-hop>\\n\\nTable 1: ChatGPT daily message cou...   \n",
              "10  [<1-hop>\\n\\nTable 1: ChatGPT daily message cou...   \n",
              "11  [<1-hop>\\n\\nTable 1: ChatGPT daily message cou...   \n",
              "12  [<1-hop>\\n\\nTable 1: ChatGPT daily message cou...   \n",
              "13  [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
              "14  [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   The provided context does not specify any deta...   \n",
              "1   The context explains that OpenAI's ChatGPT is ...   \n",
              "2   Users in nonprofessional occupations, includin...   \n",
              "3   The paper indicates that in the US, ChatGPT us...   \n",
              "4   In June 2024, non-work messages accounted for ...   \n",
              "5   The first context segment highlights that by J...   \n",
              "6   The context indicates that ChatGPT has experie...   \n",
              "7   In June 2024, non-work messages accounted for ...   \n",
              "8   The data shows that total messages increased f...   \n",
              "9   Between June 2024 and June 2025, ChatGPT exper...   \n",
              "10  The context indicates that by June 2024, ChatG...   \n",
              "11  The context highlights that the US is signific...   \n",
              "12  The context indicates that in the US, ChatGPT ...   \n",
              "13  ChatGPT was launched in November 2022. By July...   \n",
              "14  By July 2025, ChatGPT had experienced unpreced...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_abstract_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  \n",
              "12  multi_hop_specific_query_synthesizer  \n",
              "13  multi_hop_specific_query_synthesizer  \n",
              "14  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset_df = testset.to_pandas()\n",
        "testset_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# store testset_df as a csv file \n",
        "# testset_df.to_csv(\"testset.csv\", index=False)\n",
        "\n",
        "# load testset_df from csv file\n",
        "testset_df = pd.read_csv(\"testset.csv\")\n",
        "\n",
        "# Convert string representations of lists back to actual lists\n",
        "import ast\n",
        "testset_df['reference_contexts'] = testset_df['reference_contexts'].apply(ast.literal_eval)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluations Part I: Retrievers with Recursive Text Split Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: naive retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: naive_retriever_eval\n",
            "ðŸ“ LangSmith Project: naive_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for naive completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60c19a800ced439f94b63e9172845536",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9417, 'context_precision': 0.8886, 'context_entity_recall': 0.4262}\n",
            "âœ… RAGAS evaluation completed in 62.90s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: naive\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9417\n",
            "  context_precision................................. 0.8886\n",
            "  context_entity_recall............................. 0.4262\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000284\n",
            "  Prompt cost per query (USD):............................ $0.000239\n",
            "  Completion cost per query (USD):............................ $0.000046\n",
            "  Latency (P50):............................ 3.075s\n",
            "  Latency (P99):............................ 3.788s\n",
            "  Tokens per query:............................ 2499.940\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: bm25 retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: bm25_retriever_eval\n",
            "ðŸ“ LangSmith Project: bm25_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for bm25 completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd89e582933f4baebef312ce3861f485",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9167, 'context_precision': 0.7740, 'context_entity_recall': 0.4483}\n",
            "âœ… RAGAS evaluation completed in 46.44s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: bm25\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9167\n",
            "  context_precision................................. 0.7740\n",
            "  context_entity_recall............................. 0.4483\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000276\n",
            "  Prompt cost per query (USD):............................ $0.000234\n",
            "  Completion cost per query (USD):............................ $0.000042\n",
            "  Latency (P50):............................ 3.197s\n",
            "  Latency (P99):............................ 4.529s\n",
            "  Tokens per query:............................ 2447.850\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: compression retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ“ LangSmith Project: compression_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for compression completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a2ca4a7e8a5471d8b94909503d2c4e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.8833, 'context_precision': 0.9462, 'context_entity_recall': 0.3885}\n",
            "âœ… RAGAS evaluation completed in 23.61s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: compression\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.8833\n",
            "  context_precision................................. 0.9462\n",
            "  context_entity_recall............................. 0.3885\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000211\n",
            "  Prompt cost per query (USD):............................ $0.000166\n",
            "  Completion cost per query (USD):............................ $0.000045\n",
            "  Latency (P50):............................ 3.396s\n",
            "  Latency (P99):............................ 5.691s\n",
            "  Tokens per query:............................ 1770.140\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: multi_query retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: multi_query_retriever_eval\n",
            "ðŸ“ LangSmith Project: multi_query_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for multi_query completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abeb9b0da6a74f5c96642a41481e8fd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[20]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9750, 'context_precision': 0.8484, 'context_entity_recall': 0.3714}\n",
            "âœ… RAGAS evaluation completed in 188.04s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: multi_query\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9750\n",
            "  context_precision................................. 0.8484\n",
            "  context_entity_recall............................. nan\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000241\n",
            "  Prompt cost per query (USD):............................ $0.000196\n",
            "  Completion cost per query (USD):............................ $0.000045\n",
            "  Latency (P50):............................ 4.635s\n",
            "  Latency (P99):............................ 8.972s\n",
            "  Tokens per query:............................ 2075.500\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: parent_document retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: parent_document_retriever_eval\n",
            "ðŸ“ LangSmith Project: parent_document_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for parent_document completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ab334f5f7f947cd896d6c99decf3910",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.8300, 'context_precision': 0.8917, 'context_entity_recall': 0.3049}\n",
            "âœ… RAGAS evaluation completed in 20.92s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: parent_document\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.8300\n",
            "  context_precision................................. 0.8917\n",
            "  context_entity_recall............................. 0.3049\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000112\n",
            "  Prompt cost per query (USD):............................ $0.000077\n",
            "  Completion cost per query (USD):............................ $0.000035\n",
            "  Latency (P50):............................ 2.708s\n",
            "  Latency (P99):............................ 3.677s\n",
            "  Tokens per query:............................ 859.800\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: ensemble retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: ensemble_retriever_eval\n",
            "ðŸ“ LangSmith Project: ensemble_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for ensemble completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f703fbd3d884afd821e0f86e4b2343c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[23]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9750, 'context_precision': 0.8103, 'context_entity_recall': 0.3642}\n",
            "âœ… RAGAS evaluation completed in 191.31s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: ensemble\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9750\n",
            "  context_precision................................. 0.8103\n",
            "  context_entity_recall............................. nan\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000233\n",
            "  Prompt cost per query (USD):............................ $0.000195\n",
            "  Completion cost per query (USD):............................ $0.000038\n",
            "  Latency (P50):............................ 5.584s\n",
            "  Latency (P99):............................ 9.635s\n",
            "  Tokens per query:............................ 2046.700\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time \n",
        "from retriever_evaluator import evaluate_retriever, SUGGESTED_METRICS, get_suggested_metrics, RetrieverType, print_results\n",
        "\n",
        "list_of_retrievers = [\n",
        "    (RetrieverType.NAIVE, naive_retrieval_chain_updated),\n",
        "    (RetrieverType.BM25, bm25_retrieval_chain_updated),\n",
        "    (RetrieverType.COMPRESSION, contextual_compression_retrieval_chain_updated),\n",
        "    (RetrieverType.MULTI_QUERY, multi_query_retrieval_chain_updated),\n",
        "    (RetrieverType.PARENT_DOCUMENT, parent_document_retrieval_chain_updated),\n",
        "    (RetrieverType.ENSEMBLE, ensemble_retrieval_chain_updated)\n",
        "]\n",
        "\n",
        "\n",
        "final_results = {}\n",
        "for i, (retriever_type, chain) in enumerate(list_of_retrievers):\n",
        "    result = evaluate_retriever(\n",
        "        retriever_chain=chain,\n",
        "        name=retriever_type,\n",
        "        testset_df=testset_df,\n",
        "        evaluator_llm=generator_llm,\n",
        "        metrics=get_suggested_metrics(retriever_type)\n",
        "    )\n",
        "    print_results(result)\n",
        "    final_results[retriever_type] = result\n",
        "\n",
        "        # Add delay between retrievers (wait 60 seconds after every batch)\n",
        "    if i < len(list_of_retrievers) - 1:  # Don't wait after the last one\n",
        "        print(f\"Waiting 90 seconds to avoid rate limits...\")\n",
        "        time.sleep(90)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "def print_metrics_results_table(final_results: dict[str, Any]):\n",
        "    \"Print evaluation results in a formatted table\"\n",
        "    \n",
        "    all_data = []\n",
        "    \n",
        "    # Mapping for shorter column names\n",
        "    metric_name_map = {\n",
        "        'context_recall': 'Recall',\n",
        "        'context_precision': 'Precision',\n",
        "        'context_entity_recall': 'Entity_Recall'\n",
        "    }\n",
        "    \n",
        "    for retriever_type, result in final_results.items():\n",
        "        row = {'Retriever': result['name']}\n",
        "        \n",
        "        # Add quality metrics with shorter names\n",
        "        for metric_name, score in result['quality_metrics'].items():\n",
        "            short_name = metric_name_map[metric_name] if metric_name in metric_name_map else metric_name\n",
        "            row[short_name] = score\n",
        "        \n",
        "        # Add performance metrics with shorter names\n",
        "        perf = result['performance_metrics']\n",
        "        row['Cost ($)'] = perf['cost_per_query_usd']\n",
        "        row['Prompt ($)'] = perf['prompt_cost_per_query_usd']\n",
        "        row['Compl ($)'] = perf['completion_cost_per_query_usd']\n",
        "        row['Tokens'] = perf['total_tokens_per_query']\n",
        "        row['P50 (s)'] = perf['p50_latency_seconds']\n",
        "        row['P99 (s)'] = perf['p99_latency_seconds']\n",
        "        \n",
        "        all_data.append(row)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    results_df = pd.DataFrame(all_data)\n",
        "    \n",
        "    # Reorder columns: Retriever, Quality metrics, Cost metrics, Latency metrics, Tokens\n",
        "    desired_order = [\n",
        "        'Retriever',\n",
        "        'Recall', 'Precision', 'Entity_Recall',  # Quality metrics together\n",
        "        'Cost ($)', 'Prompt ($)', 'Compl ($)',   # Cost metrics together\n",
        "        'P50 (s)', 'P99 (s)',                     # Latency metrics together\n",
        "        'Tokens'                                   # Tokens last\n",
        "    ]\n",
        "    \n",
        "    # Only include columns that actually exist in the dataframe\n",
        "    column_order = [col for col in desired_order if col in results_df.columns]\n",
        "    results_df = results_df[column_order]\n",
        "    \n",
        "    # Convert Tokens to integer\n",
        "    if 'Tokens' in results_df.columns:\n",
        "        results_df['Tokens'] = results_df['Tokens'].astype(int)\n",
        "    \n",
        "    # Print with nice formatting\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\"ðŸ“Š COMPLETE EVALUATION RESULTS\")\n",
        "    print(\"=\"*120 + \"\\n\")\n",
        "    print(results_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))  # type: ignore\n",
        "    print(\"\\n\" + \"=\"*120 + \"\\n\")\n",
        "    \n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================================================================================================\n",
            "ðŸ“Š COMPLETE EVALUATION RESULTS\n",
            "========================================================================================================================\n",
            "\n",
            "      Retriever  Recall  Precision  Entity_Recall    Cost ($)  Prompt ($)   Compl ($)  P50 (s)  P99 (s)  Tokens\n",
            "          naive  0.9417     0.8886         0.4262 0.000284401 0.000238525 0.000045876   3.0750   3.7876    2499\n",
            "           bm25  0.9167     0.7740         0.4483 0.000276246 0.000234298 0.000041948   3.1975   4.5290    2447\n",
            "    compression  0.8833     0.9462         0.3885 0.000210587 0.000165823 0.000044764   3.3960   5.6913    1770\n",
            "    multi_query  0.9750     0.8484            NaN 0.000241135 0.000196355  0.00004478   4.6345   8.9723    2075\n",
            "parent_document  0.8300     0.8917         0.3049 0.000112173 0.000077249 0.000034924   2.7085   3.6767     859\n",
            "       ensemble  0.9750     0.8103            NaN 0.000233305 0.000195125  0.00003818   5.5840   9.6349    2046\n",
            "\n",
            "========================================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Re-run with updated shorter column names\n",
        "results_df = print_metrics_results_table(final_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluations Part II: Retrievers with Semantic Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Creating semantic chunks from combined dataset...\n",
            "âœ… Created 88 semantic chunks from 84 documents\n",
            "ðŸ”„ Creating vectorstore with semantic chunks...\n",
            "âœ… Semantic vectorstore created\n"
          ]
        }
      ],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "# Create semantic chunker\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"standard_deviation\",\n",
        ")\n",
        "\n",
        "# Apply semantic chunking to all documents\n",
        "print(\"ðŸ”„ Creating semantic chunks from combined dataset...\")\n",
        "semantic_documents = semantic_chunker.split_documents(final_evaluation_docs)\n",
        "print(f\"âœ… Created {len(semantic_documents)} semantic chunks from {len(final_evaluation_docs)} documents\")\n",
        "\n",
        "# Create new vectorstore with semantic chunks\n",
        "print(\"ðŸ”„ Creating vectorstore with semantic chunks...\")\n",
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Semantic_Chunks_Vectorstore\"\n",
        ")\n",
        "print(f\"âœ… Semantic vectorstore created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Semantic naive retriever created\n",
            "âœ… Semantic BM25 retriever created\n",
            "âœ… Semantic compression retriever created\n",
            "âœ… Semantic multi-query retriever created\n",
            "âœ… Semantic parent document retriever created\n",
            "âœ… Semantic ensemble retriever created\n",
            "\n",
            "ðŸŽ‰ All semantic retrievers built with 88 semantic chunks!\n"
          ]
        }
      ],
      "source": [
        "# Rebuild naive retriever with semantic chunks\n",
        "semantic_naive_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
        "print(\"âœ… Semantic naive retriever created\")\n",
        "\n",
        "# Rebuild BM25 retriever with semantic chunks\n",
        "semantic_bm25_retriever = BM25Retriever.from_documents(semantic_documents)\n",
        "semantic_bm25_retriever.k = 10\n",
        "print(\"âœ… Semantic BM25 retriever created\")\n",
        "\n",
        "# Rebuild contextual compression retriever with semantic chunks\n",
        "semantic_compressor = CohereRerank(model=\"rerank-v3.5\", top_n=7)\n",
        "semantic_compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=semantic_compressor, \n",
        "    base_retriever=semantic_naive_retriever\n",
        ")\n",
        "print(\"âœ… Semantic compression retriever created\")\n",
        "\n",
        "# Rebuild multi-query retriever with semantic chunks\n",
        "semantic_multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=semantic_naive_retriever, \n",
        "    llm=chat_model\n",
        ")\n",
        "print(\"âœ… Semantic multi-query retriever created\")\n",
        "\n",
        "# Rebuild parent document retriever with semantic chunks\n",
        "semantic_client = QdrantClient(location=\":memory:\")\n",
        "semantic_client.create_collection(\n",
        "    collection_name=\"semantic_parent_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "semantic_parent_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"semantic_parent_documents\", \n",
        "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), \n",
        "    client=semantic_client\n",
        ")\n",
        "\n",
        "semantic_parent_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=semantic_parent_vectorstore,\n",
        "    docstore=InMemoryStore(),\n",
        "    child_splitter=child_splitter,\n",
        ")\n",
        "semantic_parent_retriever.add_documents(semantic_documents, ids=None)\n",
        "print(\"âœ… Semantic parent document retriever created\")\n",
        "\n",
        "# Rebuild ensemble retriever with semantic chunks\n",
        "semantic_retriever_list = [\n",
        "    semantic_bm25_retriever, \n",
        "    semantic_naive_retriever, \n",
        "    semantic_parent_retriever, \n",
        "    semantic_compression_retriever, \n",
        "    semantic_multi_query_retriever\n",
        "]\n",
        "semantic_equal_weighting = [1/len(semantic_retriever_list)] * len(semantic_retriever_list)\n",
        "\n",
        "semantic_ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=semantic_retriever_list, \n",
        "    weights=semantic_equal_weighting\n",
        ")\n",
        "print(\"âœ… Semantic ensemble retriever created\")\n",
        "print(f\"\\nðŸŽ‰ All semantic retrievers built with {len(semantic_documents)} semantic chunks!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Building RAG chains with semantic retrievers...\n",
            "âœ… All semantic RAG chains built!\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ”„ Building RAG chains with semantic retrievers...\")\n",
        "\n",
        "# Naive retrieval chain with semantic chunks\n",
        "semantic_naive_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# BM25 retrieval chain with semantic chunks\n",
        "semantic_bm25_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Contextual compression chain with semantic chunks\n",
        "semantic_compression_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Multi-query chain with semantic chunks\n",
        "semantic_multi_query_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Parent document chain with semantic chunks\n",
        "semantic_parent_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_parent_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "# Ensemble chain with semantic chunks\n",
        "semantic_ensemble_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "print(\"âœ… All semantic RAG chains built!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: naive retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: naive_retriever_eval\n",
            "ðŸ“ LangSmith Project: naive_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for naive completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c75509d7539473b8882c5757ec15efb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9417, 'context_precision': 0.8736, 'context_entity_recall': 0.4025}\n",
            "âœ… RAGAS evaluation completed in 40.73s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: naive\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9417\n",
            "  context_precision................................. 0.8736\n",
            "  context_entity_recall............................. 0.4025\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000278\n",
            "  Prompt cost per query (USD):............................ $0.000232\n",
            "  Completion cost per query (USD):............................ $0.000046\n",
            "  Latency (P50):............................ 3.176s\n",
            "  Latency (P99):............................ 5.340s\n",
            "  Tokens per query:............................ 2435.340\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: bm25 retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: bm25_retriever_eval\n",
            "ðŸ“ LangSmith Project: bm25_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for bm25 completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac44782ae9fe449c8bbd731256f6da58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9500, 'context_precision': 0.7507, 'context_entity_recall': 0.4214}\n",
            "âœ… RAGAS evaluation completed in 41.45s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: bm25\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9500\n",
            "  context_precision................................. 0.7507\n",
            "  context_entity_recall............................. 0.4214\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000275\n",
            "  Prompt cost per query (USD):............................ $0.000229\n",
            "  Completion cost per query (USD):............................ $0.000047\n",
            "  Latency (P50):............................ 3.152s\n",
            "  Latency (P99):............................ 4.499s\n",
            "  Tokens per query:............................ 2402.790\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: compression retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: compression_retriever_eval\n",
            "ðŸ“ LangSmith Project: compression_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for compression completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cac22e1eb05408a8fcdd212971099fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9417, 'context_precision': 0.9111, 'context_entity_recall': 0.3536}\n",
            "âœ… RAGAS evaluation completed in 24.54s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: compression\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9417\n",
            "  context_precision................................. 0.9111\n",
            "  context_entity_recall............................. 0.3536\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000206\n",
            "  Prompt cost per query (USD):............................ $0.000162\n",
            "  Completion cost per query (USD):............................ $0.000043\n",
            "  Latency (P50):............................ 4.148s\n",
            "  Latency (P99):............................ 7.103s\n",
            "  Tokens per query:............................ 1732.130\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: multi_query retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: multi_query_retriever_eval\n",
            "ðŸ“ LangSmith Project: multi_query_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for multi_query completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50b2014348f64f529df14a587a1f792e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9417, 'context_precision': 0.8787, 'context_entity_recall': 0.3419}\n",
            "âœ… RAGAS evaluation completed in 65.57s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: multi_query\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9417\n",
            "  context_precision................................. 0.8787\n",
            "  context_entity_recall............................. 0.3419\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000229\n",
            "  Prompt cost per query (USD):............................ $0.000181\n",
            "  Completion cost per query (USD):............................ $0.000048\n",
            "  Latency (P50):............................ 4.757s\n",
            "  Latency (P99):............................ 7.022s\n",
            "  Tokens per query:............................ 1987.940\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: parent_document retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: parent_document_retriever_eval\n",
            "ðŸ“ LangSmith Project: parent_document_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for parent_document completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2eb3de1bca8465e8e19ecf050ef14b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.8500, 'context_precision': 0.8944, 'context_entity_recall': 0.2988}\n",
            "âœ… RAGAS evaluation completed in 18.05s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: parent_document\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.8500\n",
            "  context_precision................................. 0.8944\n",
            "  context_entity_recall............................. 0.2988\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000124\n",
            "  Prompt cost per query (USD):............................ $0.000087\n",
            "  Completion cost per query (USD):............................ $0.000037\n",
            "  Latency (P50):............................ 2.531s\n",
            "  Latency (P99):............................ 3.607s\n",
            "  Tokens per query:............................ 960.320\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Waiting 90 seconds to avoid rate limits...\n",
            "\n",
            "======================================================================\n",
            "ðŸ” Evaluating: ensemble retriever\n",
            "======================================================================\n",
            "\n",
            "ðŸ—‘ï¸  Deleted existing project: ensemble_retriever_eval\n",
            "ðŸ“ LangSmith Project: ensemble_retriever_eval\n",
            "\n",
            "ðŸ“Š Retrieving for 10 test questions...\n",
            " Retrieval for ensemble completed for 10 eval samples.\n",
            "\n",
            "Running RAGAS evaluation ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95af522dba0f44c4a18974cccbf21e34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.9750, 'context_precision': 0.7945, 'context_entity_recall': 0.4492}\n",
            "âœ… RAGAS evaluation completed in 117.21s\n",
            "\n",
            "ðŸ’° Fetching cost and latency from LangSmith...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š RESULTS: ensemble\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ QUALITY METRICS (RAGAS):\n",
            "----------------------------------------------------------------------\n",
            "  context_recall.................................... 0.9750\n",
            "  context_precision................................. 0.7945\n",
            "  context_entity_recall............................. 0.4492\n",
            "\n",
            " âš¡ COST and LATENCY METRICS:\n",
            "----------------------------------------------------------------------\n",
            "  Cost per query (USD):............................ $0.000224\n",
            "  Prompt cost per query (USD):............................ $0.000183\n",
            "  Completion cost per query (USD):............................ $0.000041\n",
            "  Latency (P50):............................ 5.793s\n",
            "  Latency (P99):............................ 9.271s\n",
            "  Tokens per query:............................ 1928.650\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "semantic_retrievers_list = [\n",
        "    (RetrieverType.NAIVE, semantic_naive_chain),\n",
        "    (RetrieverType.BM25, semantic_bm25_chain),\n",
        "    (RetrieverType.COMPRESSION, semantic_compression_chain),\n",
        "    (RetrieverType.MULTI_QUERY, semantic_multi_query_chain),\n",
        "    (RetrieverType.PARENT_DOCUMENT, semantic_parent_chain),\n",
        "    (RetrieverType.ENSEMBLE, semantic_ensemble_chain)\n",
        "]\n",
        "\n",
        "semantic_results = {}\n",
        "for i, (retriever_type, chain) in enumerate(semantic_retrievers_list):\n",
        "    result = evaluate_retriever(\n",
        "        retriever_chain=chain,\n",
        "        name=retriever_type,\n",
        "        testset_df=testset_df,\n",
        "        evaluator_llm=generator_llm,\n",
        "        metrics=get_suggested_metrics(retriever_type)\n",
        "    )\n",
        "    print_results(result)\n",
        "    semantic_results[retriever_type] = result\n",
        "    \n",
        "    # Add delay between retrievers to avoid rate limits\n",
        "    if i < len(semantic_retrievers_list) - 1:\n",
        "        print(f\"Waiting 90 seconds to avoid rate limits...\")\n",
        "        time.sleep(90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================================================================================================\n",
            "ðŸ“Š COMPLETE EVALUATION RESULTS\n",
            "========================================================================================================================\n",
            "\n",
            "      Retriever  Recall  Precision  Entity_Recall    Cost ($)  Prompt ($)   Compl ($)  P50 (s)  P99 (s)  Tokens\n",
            "          naive  0.9417     0.8736         0.4025 0.000278139 0.000231999  0.00004614   3.1760   5.3404    2435\n",
            "           bm25  0.9500     0.7507         0.4214 0.000275427 0.000228563 0.000046864   3.1525   4.4986    2402\n",
            "    compression  0.9417     0.9111         0.3536 0.000205769 0.000162361 0.000043408   4.1480   7.1026    1732\n",
            "    multi_query  0.9417     0.8787         0.3419 0.000228875 0.000181087 0.000047788   4.7570   7.0220    1987\n",
            "parent_document  0.8500     0.8944         0.2988 0.000124139 0.000086663 0.000037476   2.5310   3.6066     960\n",
            "       ensemble  0.9750     0.7945         0.4492 0.000223561 0.000182633 0.000040928   5.7925   9.2705    1928\n",
            "\n",
            "========================================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display results in a formatted table\n",
        "semantic_results_df = print_metrics_results_table(semantic_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================================================================================================\n",
            "ðŸ“Š ORIGINAL vs SEMANTIC CHUNKING COMPARISON\n",
            "========================================================================================================================\n",
            "\n",
            "      Retriever            Chunking Recall Precision Entity_Recall Cost ($) P50 (s) P99 (s) Tokens\n",
            "          naive Recursive Splitting 0.9417    0.8886        0.4262   0.0003  3.0750  3.7876   2499\n",
            "          naive            Semantic 0.9417    0.8736        0.4025   0.0003  3.1760  5.3404   2435\n",
            "           bm25 Recursive Splitting 0.9167    0.7740        0.4483   0.0003  3.1975  4.5290   2447\n",
            "           bm25            Semantic 0.9500    0.7507        0.4214   0.0003  3.1525  4.4986   2402\n",
            "    compression Recursive Splitting 0.8833    0.9462        0.3885   0.0002  3.3960  5.6913   1770\n",
            "    compression            Semantic 0.9417    0.9111        0.3536   0.0002  4.1480  7.1026   1732\n",
            "    multi_query Recursive Splitting 0.9750    0.8484           N/A   0.0002  4.6345  8.9723   2075\n",
            "    multi_query            Semantic 0.9417    0.8787        0.3419   0.0002  4.7570  7.0220   1987\n",
            "parent_document Recursive Splitting 0.8300    0.8917        0.3049   0.0001  2.7085  3.6767    859\n",
            "parent_document            Semantic 0.8500    0.8944        0.2988   0.0001  2.5310  3.6066    960\n",
            "       ensemble Recursive Splitting 0.9750    0.8103           N/A   0.0002  5.5840  9.6349   2046\n",
            "       ensemble            Semantic 0.9750    0.7945        0.4492   0.0002  5.7925  9.2705   1928\n",
            "\n",
            "========================================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create comparison table with consecutive rows for Original vs Semantic\n",
        "comparison_data = []\n",
        "\n",
        "for retriever_type in final_results.keys():\n",
        "    if retriever_type in semantic_results:\n",
        "        orig = final_results[retriever_type]\n",
        "        sem = semantic_results[retriever_type]\n",
        "        \n",
        "        # Original row\n",
        "        orig_row = {\n",
        "            'Retriever': retriever_type,\n",
        "            'Chunking': 'Recursive Splitting',\n",
        "            'Recall': orig['quality_metrics'].get('context_recall', None),\n",
        "            'Precision': orig['quality_metrics'].get('context_precision', None),\n",
        "            'Entity_Recall': orig['quality_metrics'].get('context_entity_recall', None),\n",
        "            'Cost ($)': orig['performance_metrics']['cost_per_query_usd'],\n",
        "            'P50 (s)': orig['performance_metrics']['p50_latency_seconds'],\n",
        "            'P99 (s)': orig['performance_metrics']['p99_latency_seconds'],\n",
        "            'Tokens': int(orig['performance_metrics']['total_tokens_per_query']),\n",
        "        }\n",
        "        \n",
        "        # Semantic row\n",
        "        sem_row = {\n",
        "            'Retriever': retriever_type,\n",
        "            'Chunking': 'Semantic',\n",
        "            'Recall': sem['quality_metrics'].get('context_recall', None),\n",
        "            'Precision': sem['quality_metrics'].get('context_precision', None),\n",
        "            'Entity_Recall': sem['quality_metrics'].get('context_entity_recall', None),\n",
        "            'Cost ($)': sem['performance_metrics']['cost_per_query_usd'],\n",
        "            'P50 (s)': sem['performance_metrics']['p50_latency_seconds'],\n",
        "            'P99 (s)': sem['performance_metrics']['p99_latency_seconds'],\n",
        "            'Tokens': int(sem['performance_metrics']['total_tokens_per_query']),\n",
        "        }\n",
        "        \n",
        "        comparison_data.append(orig_row)\n",
        "        comparison_data.append(sem_row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*120)\n",
        "print(\"ðŸ“Š ORIGINAL vs SEMANTIC CHUNKING COMPARISON\")\n",
        "print(\"=\"*120 + \"\\n\")\n",
        "\n",
        "# Custom formatting function\n",
        "def format_value(val):\n",
        "    if pd.isna(val):\n",
        "        return 'N/A'\n",
        "    elif isinstance(val, (int, np.integer)):\n",
        "        return f'{val}'\n",
        "    else:\n",
        "        return f'{val:.4f}'\n",
        "\n",
        "# Apply formatting for display\n",
        "display_df = comparison_df.copy()\n",
        "for col in display_df.columns:\n",
        "    if col not in ['Retriever', 'Chunking']:\n",
        "        display_df[col] = display_df[col].apply(format_value)\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*120 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Conclusions**\n",
        "\n",
        "**Note**: High recall minimizes Type II error (false negatives, i.e., avoid missing relevant docs), while high precision minimizes Type I error (false positives, i.e., higher fraction of retrieved docs are relevant).\n",
        "\n",
        "- In terms of recall, ensemble and multi-query retrievers perform best (on average).\n",
        "\n",
        "    - These retirevers cast a wider net, capturing more relevant docs in context\n",
        "\n",
        "- Compression retrievers were the clear winner in terms of precision.\n",
        "\n",
        "    - Reranking filters out irrelevant docs during retrieval, minimize Type I errors (false positives)\n",
        "\n",
        "- Parent document retrievers incurred the lowest costs and latencies.\n",
        "\n",
        "    - This is because they return fewer larger chunks v/s others (even though chunk size of the retrieved whole document may be higher in isolation)\n",
        "\n",
        "- BM25 struggles with precision \n",
        "\n",
        "    - keyword mathcing retrieves more loosely related docs\n",
        "\n",
        "- Semantic v/s Recursive Chunking\n",
        "\n",
        "| Metric | Trend | Explanation |\n",
        "|--------|-------|-------------|\n",
        "| **Recall** | Semantic is slightly better | Semantic chunks preserve topic coherence, improving retrieval of related content |\n",
        "| **Precision** | Recursive slightly better | Fixed-size recursive chunks may be more focused without including adjacent context |\n",
        "| **Cost/Tokens** | Reduces tokens in general | Slightly more efficient chunking in terms of next-token predictions |\n",
        "| **Latency** | Mixed  | Semantic chunking doesn't significantly impact retrieval speed |\n",
        "\n",
        "**Summary**: Semantic chunking improves recall (finds more relevant docs) but slightly reduces precision (includes more noise). Token/cost savings are marginal."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
