{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a LangGraph Agent that Uses A2A Protocol\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build a **LangGraph-based agent** that communicates with an A2A-compliant agent service. This is an example of **agent-to-agent (A2A) communication** where one agent can use another agent as a tool.\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "1. **A2A Protocol Integration**: How to connect to and discover A2A agent capabilities\n",
        "2. **Tool Creation**: How to wrap an A2A client as a LangChain tool\n",
        "3. **LangGraph Agent**: How to build an agent that autonomously decides when to delegate tasks\n",
        "4. **Async Execution**: Proper async/await patterns for non-blocking I/O\n",
        "5. **Multi-Turn Conversations**: How to maintain context across multiple interactions\n",
        "\n",
        "### Architecture\n",
        "\n",
        "```\n",
        "User Query\n",
        "    ‚Üì\n",
        "LangGraph Agent (Client)\n",
        "    ‚Üì\n",
        "  Decision: Use tool or answer directly?\n",
        "    ‚Üì\n",
        "A2A Protocol (HTTP/JSON-RPC)\n",
        "    ‚Üì\n",
        "A2A Agent Server (localhost:10000)\n",
        "    ‚îú‚îÄ Web Search\n",
        "    ‚îú‚îÄ ArXiv Search  \n",
        "    ‚îî‚îÄ RAG Search\n",
        "    ‚Üì\n",
        "Response to User\n",
        "```\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "**Before running this notebook:**\n",
        "\n",
        "1. **Start the A2A agent service:**\n",
        "   ```bash\n",
        "   cd app && uv run python -m app\n",
        "   ```\n",
        "\n",
        "2. **Verify it's running:**\n",
        "   - The server should be accessible at http://localhost:10000\n",
        "   - You should see log output indicating the server started successfully\n",
        "\n",
        "3. **Environment setup:**\n",
        "   - Ensure your `.env` file contains `OPENAI_API_KEY`\n",
        "   - Optionally set `TOOL_LLM_NAME` (defaults to `gpt-4o-mini`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "We'll import all necessary libraries for building our agent. The A2A protocol specific imports include\n",
        "\n",
        "### A2A Protocol Imports\n",
        "- **`httpx`**: Async HTTP client (required for non-blocking I/O)\n",
        "- **`A2ACardResolver`**: Discovers agent capabilities via AgentCards\n",
        "- **`ClientFactory, ClientConfig`**: Creates properly configured A2A clients\n",
        "\n",
        "### Why Async?\n",
        "The A2A client uses `httpx.AsyncClient` for async HTTP requests. This means:\n",
        "- Our tool must be async (`async def`)\n",
        "- We use `await` for async operations\n",
        "- LangGraph must use `astream()` instead of `stream()`\n",
        "- Better performance through non-blocking I/O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "import httpx\n",
        "from a2a.client import A2ACardResolver, ClientFactory, ClientConfig\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize the A2A Client\n",
        "\n",
        "This step establishes a connection to the A2A agent and discovers its capabilities.\n",
        "\n",
        "### What Happens Here:\n",
        "\n",
        "1. **Create Async HTTP Client**\n",
        "   ```python\n",
        "   httpx_client = httpx.AsyncClient(timeout=60.0)\n",
        "   ```\n",
        "   - Timeout set to 60 seconds (LLMs can take time for complex queries)\n",
        "   - Async client allows non-blocking operations\n",
        "\n",
        "2. **Create AgentCard Resolver**\n",
        "   ```python\n",
        "   resolver = A2ACardResolver(...)\n",
        "   ```\n",
        "   - AgentCard = machine-readable description of agent capabilities (like OpenAPI spec)\n",
        "   - Located at `/.well-known/agent-card.json` (standard A2A endpoint)\n",
        "\n",
        "3. **Fetch the AgentCard**\n",
        "   ```python\n",
        "   agent_card = await resolver.get_agent_card()\n",
        "   ```\n",
        "   - Makes HTTP GET request to discover agent\n",
        "   - Returns structured data about:\n",
        "     - Agent name and description\n",
        "     - Available skills (web search, arxiv, RAG)\n",
        "     - Capabilities (streaming, notifications)\n",
        "     - Protocol version and transport method\n",
        "\n",
        "4. **Create A2A Client**\n",
        "   ```python\n",
        "   factory = ClientFactory(...)\n",
        "   a2a_client = factory.create(card=agent_card)\n",
        "   ```\n",
        "   - Factory pattern creates properly configured client\n",
        "   - Client knows how to communicate with this specific agent\n",
        "   - Ready to send messages and receive responses\n",
        "\n",
        "### Expected Output:\n",
        "You should see the agent name and its available skills:\n",
        "- Web Search Tool\n",
        "- Academic Paper Search\n",
        "- Document Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: GET http://localhost:10000/.well-known/agent-card.json \"HTTP/1.1 200 OK\"\n",
            "INFO:a2a.client.card_resolver:Successfully fetched agent card data from http://localhost:10000/.well-known/agent-card.json: {'capabilities': {'pushNotifications': True, 'streaming': True}, 'defaultInputModes': ['text', 'text/plain'], 'defaultOutputModes': ['text', 'text/plain'], 'description': 'A helpful AI assistant with web search, academic paper search, and document retrieval capabilities', 'name': 'General Purpose Agent', 'preferredTransport': 'JSONRPC', 'protocolVersion': '0.3.0', 'skills': [{'description': 'Search the web for current information', 'examples': ['What are the latest news about AI?'], 'id': 'web_search', 'name': 'Web Search Tool', 'tags': ['search', 'web', 'internet']}, {'description': 'Search for academic papers on arXiv', 'examples': ['Find recent papers on large language models'], 'id': 'arxiv_search', 'name': 'Academic Paper Search', 'tags': ['research', 'papers', 'academic']}, {'description': 'Search through loaded documents for specific information', 'examples': ['What do the policy documents say about student loans?'], 'id': 'rag_search', 'name': 'Document Retrieval', 'tags': ['documents', 'rag', 'retrieval']}], 'url': 'http://localhost:10000/', 'version': '1.0.0'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Connected to: General Purpose Agent\n",
            "   Skills: ['Web Search Tool', 'Academic Paper Search', 'Document Retrieval']\n"
          ]
        }
      ],
      "source": [
        "# Initialize A2A client\n",
        "A2A_URL = 'http://localhost:10000'\n",
        "httpx_client = httpx.AsyncClient(timeout=httpx.Timeout(60.0))\n",
        "resolver = A2ACardResolver(httpx_client=httpx_client, base_url=A2A_URL)\n",
        "agent_card = await resolver.get_agent_card()\n",
        "factory = ClientFactory(ClientConfig(httpx_client=httpx_client))\n",
        "a2a_client = factory.create(card=agent_card)\n",
        "\n",
        "print(f\"‚úÖ Connected to: {agent_card.name}\")\n",
        "print(f\"   Skills: {[s.name for s in agent_card.skills]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create the A2A Tool\n",
        "\n",
        "This is the **most important step** - we wrap the A2A client as a LangChain tool that our LangGraph agent can use.\n",
        "\n",
        "### The `@tool` Decorator\n",
        "\n",
        "The `@tool` decorator automatically:\n",
        "- Converts the function into a LangChain tool\n",
        "- Generates tool schema from function signature and docstring\n",
        "- Makes it available to the LLM for autonomous use\n",
        "\n",
        "The LLM sees:\n",
        "```json\n",
        "{\n",
        "  \"name\": \"query_a2a_agent\",\n",
        "  \"description\": \"Query the A2A agent (web search, arxiv, RAG).\",\n",
        "  \"parameters\": {\n",
        "    \"query\": {\"type\": \"string\", \"description\": \"Question to ask\"}\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### How It Works:\n",
        "\n",
        "1. **Construct A2A Message**\n",
        "   - Format: `{role, parts, message_id}`\n",
        "   - Each message needs unique ID (A2A protocol requirement)\n",
        "\n",
        "2. **Send Message and Stream Response**\n",
        "   - `async for chunk in a2a_client.send_message(message)`\n",
        "   - Processes streaming chunks as they arrive\n",
        "\n",
        "3. **Parse Response Structure** (Critical!)\n",
        "   - A2A responses: `(task, payload)` tuples\n",
        "   - Text location: `payload.artifact.parts[0].root.text`\n",
        "   \n",
        "   Structure:\n",
        "   ```\n",
        "   chunk (tuple)\n",
        "     ‚îú‚îÄ task (Task metadata)\n",
        "     ‚îî‚îÄ payload (TaskArtifactUpdateEvent)\n",
        "         ‚îî‚îÄ artifact (Artifact object)\n",
        "             ‚îî‚îÄ parts (List[Part])\n",
        "                 ‚îî‚îÄ [0].root.text ‚Üê THE ACTUAL TEXT!\n",
        "   ```\n",
        "\n",
        "4. **Return Result**\n",
        "   - Accumulated text from all chunks\n",
        "   - LangGraph passes this back to the LLM\n",
        "\n",
        "### Error Handling:\n",
        "- Catches network errors, parsing errors, etc.\n",
        "- Returns error as string (LLM can see what went wrong)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tool created: query_a2a_agent\n"
          ]
        }
      ],
      "source": [
        "@tool\n",
        "async def query_a2a_agent(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Query the A2A agent (web search, arxiv, RAG).\n",
        "    \n",
        "    Args:\n",
        "        query: Question to ask\n",
        "    Returns:\n",
        "        Response from A2A agent\n",
        "    \"\"\"\n",
        "    try:\n",
        "        message = {\n",
        "            \"role\": \"user\",\n",
        "            \"parts\": [{\"kind\": \"text\", \"text\": query}],\n",
        "            \"message_id\": uuid4().hex,\n",
        "        }\n",
        "        \n",
        "        logger.info(f\"Querying A2A: {query}\")\n",
        "        response_text = \"\"\n",
        "        \n",
        "        async for chunk in a2a_client.send_message(message):\n",
        "            # A2A returns (task, payload) tuples\n",
        "            if isinstance(chunk, tuple) and len(chunk) == 2:\n",
        "                task, payload = chunk\n",
        "                \n",
        "                # Check for artifact updates (where the response is)\n",
        "                if payload and hasattr(payload, 'artifact'):\n",
        "                    artifact = payload.artifact\n",
        "                    if hasattr(artifact, 'parts'):\n",
        "                        for part in artifact.parts:\n",
        "                            # Each part has a 'root' attribute with the TextPart\n",
        "                            if hasattr(part, 'root') and hasattr(part.root, 'text'):\n",
        "                                response_text += part.root.text\n",
        "        \n",
        "        result = response_text.strip()\n",
        "        logger.info(f\"Got response: {result[:100]}...\")\n",
        "        return result if result else \"No response from A2A agent\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error: {e}\")\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "print(f\"‚úÖ Tool created: {query_a2a_agent.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build the LangGraph Agent\n",
        "\n",
        "Now we create the **decision-making agent** that autonomously chooses when to use the A2A tool.\n",
        "\n",
        "### Components:\n",
        "\n",
        "#### 1. **Initialize the LLM**\n",
        "```python\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "```\n",
        "- Temperature 0 = deterministic responses (no randomness)\n",
        "- This is the \"brain\" that makes decisions\n",
        "\n",
        "#### 2. **Bind Tools to LLM**\n",
        "```python\n",
        "llm_with_tools = llm.bind_tools([query_a2a_agent])\n",
        "```\n",
        "- LLM can now \"see\" the A2A tool\n",
        "- Decides: \"Should I use this tool or answer directly?\"\n",
        "\n",
        "#### 3. **Create Memory**\n",
        "```python\n",
        "memory = MemorySaver()\n",
        "```\n",
        "- Stores conversation history\n",
        "- Each conversation has unique `thread_id`\n",
        "- Enables multi-turn context\n",
        "\n",
        "### Graph Architecture:\n",
        "\n",
        "```\n",
        "START\n",
        "  ‚Üì\n",
        "agent (LLM analyzes & decides)\n",
        "  ‚Üì\n",
        "  ‚îú‚îÄ‚Üí tools (execute A2A query)\n",
        "  ‚îÇ      ‚Üì\n",
        "  ‚îÇ    agent (process results)\n",
        "  ‚îÇ      ‚Üì\n",
        "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚Üí END (return answer)\n",
        "```\n",
        "\n",
        "### Decision Flow Example:\n",
        "\n",
        "**User asks:** \"What are the latest AI developments?\"\n",
        "\n",
        "1. **Agent node**: LLM thinks \"I need current info ‚Üí use tool\"\n",
        "2. **Tools node**: Execute `query_a2a_agent(\"latest AI developments\")`\n",
        "3. **Agent node**: LLM formats the response nicely\n",
        "4. **END**: Return formatted answer\n",
        "\n",
        "**User asks:** \"What is 2+2?\"\n",
        "\n",
        "1. **Agent node**: LLM thinks \"I know this ‚Üí answer directly\"\n",
        "2. **END**: Return \"2 + 2 equals 4\"\n",
        "\n",
        "### Routing Logic:\n",
        "\n",
        "The `should_continue` function decides the next step:\n",
        "- **Has tool_calls?** ‚Üí Go to \"tools\" node\n",
        "- **No tool_calls?** ‚Üí Go to \"end\" (we're done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LangGraph agent ready!\n"
          ]
        }
      ],
      "source": [
        "# Build LangGraph agent\n",
        "llm = ChatOpenAI(model=os.getenv('TOOL_LLM_NAME', 'gpt-4o-mini'), temperature=0)\n",
        "tools = [query_a2a_agent]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "memory = MemorySaver()\n",
        "\n",
        "def agent_node(state: MessagesState) -> MessagesState:\n",
        "    \"\"\"Agent node: LLM analyzes messages and decides what to do.\"\"\"\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "def should_continue(state: MessagesState) -> str:\n",
        "    \"\"\"Routing logic: Continue to tools or end?\"\"\"\n",
        "    last = state[\"messages\"][-1]\n",
        "    return \"tools\" if (hasattr(last, \"tool_calls\") and last.tool_calls) else \"end\"\n",
        "\n",
        "# Build the graph\n",
        "workflow = StateGraph(MessagesState)\n",
        "workflow.add_node(\"agent\", agent_node)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "workflow.add_edge(START, \"agent\")\n",
        "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", \"end\": END})\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Compile with memory for multi-turn conversations\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "print(\"‚úÖ LangGraph agent ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Helper Function for Testing\n",
        "\n",
        "The `chat()` function simplifies running queries and displaying results.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "1. **Async Streaming**: Uses `graph.astream()` (required for async tools)\n",
        "2. **Thread Management**: Each conversation has a `thread_id` for context\n",
        "3. **Pretty Display**: Shows what's happening at each step\n",
        "\n",
        "### What You'll See:\n",
        "\n",
        "- üîß **Tool calls**: When the agent decides to use the A2A tool\n",
        "- üìä **Tool results**: Response from the A2A agent (truncated if long)\n",
        "- ü§ñ **Final answer**: LLM's formatted response to the user\n",
        "\n",
        "### Important: Why `astream()`?\n",
        "\n",
        "We use `graph.astream()` instead of `graph.stream()` because:\n",
        "- Our A2A tool is async (uses `async def`)\n",
        "- Sync `stream()` would cause: `NotImplementedError: StructuredTool does not support sync invocation`\n",
        "- Async execution allows non-blocking HTTP requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Chat function ready\n"
          ]
        }
      ],
      "source": [
        "# Helper function to run tests\n",
        "async def chat(query: str, thread_id: str = \"default\"):\n",
        "    \"\"\"Send a query and display results.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üë§ User: {query}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "    input_msg = {\"messages\": [HumanMessage(content=query)]}\n",
        "    \n",
        "    # IMPORTANT: Use astream() for async tools!\n",
        "    async for chunk in graph.astream(input_msg, config, stream_mode=\"values\"):\n",
        "        if \"messages\" in chunk:\n",
        "            msg = chunk[\"messages\"][-1]\n",
        "            if isinstance(msg, AIMessage) and msg.content:\n",
        "                print(f\"\\nü§ñ Agent: {msg.content}\")\n",
        "            elif isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "                print(f\"\\nüîß Using: {[tc['name'] for tc in msg.tool_calls]}\")\n",
        "            elif isinstance(msg, ToolMessage):\n",
        "                preview = msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content\n",
        "                print(f\"\\nüìä Result: {preview}\")\n",
        "\n",
        "print(\"‚úÖ Chat function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1: Web Search Query\n",
        "\n",
        "This test demonstrates the agent using the A2A tool for web search.\n",
        "\n",
        "### What Should Happen:\n",
        "\n",
        "1. **User query**: \"What are the latest developments in large language models?\"\n",
        "2. **LLM decision**: \"This needs current information ‚Üí I should use the A2A tool\"\n",
        "3. **Tool execution**: A2A agent performs web search\n",
        "4. **Response**: Recent articles and news about LLMs\n",
        "5. **LLM formatting**: Presents results in a clear, organized way\n",
        "\n",
        "### Behind the Scenes:\n",
        "\n",
        "```\n",
        "User query\n",
        "  ‚Üì\n",
        "LangGraph Agent analyzes\n",
        "  ‚Üì\n",
        "Calls query_a2a_agent(\"latest developments in large language models 2023\")\n",
        "  ‚Üì\n",
        "A2A Protocol ‚Üí A2A Agent ‚Üí Web Search Tool ‚Üí Tavily API\n",
        "  ‚Üì\n",
        "Returns: Recent LLM news/articles\n",
        "  ‚Üì\n",
        "LLM formats into bullet points\n",
        "  ‚Üì\n",
        "Display to user\n",
        "```\n",
        "\n",
        "### Key Observation:\n",
        "Notice the LLM adds context to the query (e.g., \"2023\") to get more relevant results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üë§ User: What are the latest developments in large language models?\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:__main__:Querying A2A: latest developments in large language models 2023\n",
            "INFO:httpx:HTTP Request: POST http://localhost:10000/ \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Using: ['query_a2a_agent']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:a2a.client.client_task_manager:New task created with id: 7a9ccdc0-c995-4c0c-92e0-48a5f1fa4602\n",
            "INFO:__main__:Got response: Here are some of the latest developments in large language models (LLMs) as of 2023:\n",
            "\n",
            "1. **Ethical C...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Result: Here are some of the latest developments in large language models (LLMs) as of 2023:\n",
            "\n",
            "1. **Ethical Considerations in Psychotherapy**: A systematic rev...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Agent: Here are some of the latest developments in large language models (LLMs) as of 2023:\n",
            "\n",
            "1. **Ethical Considerations in Psychotherapy**: A systematic review has examined the opportunities and risks of using LLMs in mental health, discussing their potential benefits and harms in digital mental health applications. This includes the design of AI-based conversational agents aimed at promoting mental health and well-being.\n",
            "\n",
            "2. **Backdoor Vulnerabilities**: Research from Anthropic and other institutions has revealed that LLMs can develop backdoor vulnerabilities from a surprisingly small number of malicious documents. This study indicates that even large models can be compromised with as few as 250 corrupted documents, raising concerns about the security of training data.\n",
            "\n",
            "3. **Cognitive Decline from Low-Quality Data**: A study found that LLMs trained on low-quality, high-engagement social media content experience a decline in cognitive abilities, akin to 'brain rot'. This suggests that the quality of training data significantly impacts model performance and reliability.\n",
            "\n",
            "4. **IBM and Anthropic Partnership**: IBM announced a partnership with Anthropic to integrate their LLM, Claude, into IBM's software products. This collaboration aims to enhance enterprise software development with improved security and governance, marking a significant step in the application of LLMs in business environments.\n",
            "\n",
            "These developments reflect ongoing research and practical applications of LLMs, highlighting both their potential and the challenges they present.\n"
          ]
        }
      ],
      "source": [
        "await chat(\"What are the latest developments in large language models?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 2: Academic Paper Search\n",
        "\n",
        "This test shows the agent using ArXiv search via the A2A agent.\n",
        "\n",
        "### What Should Happen:\n",
        "\n",
        "1. **User query**: \"Find recent papers on transformer architectures\"\n",
        "2. **LLM decision**: \"This needs academic papers ‚Üí use A2A tool\"\n",
        "3. **Tool execution**: A2A agent queries ArXiv\n",
        "4. **Response**: List of recent research papers with:\n",
        "   - Title\n",
        "   - Authors\n",
        "   - Publication date\n",
        "   - Abstract/summary\n",
        "5. **LLM formatting**: Organizes papers into readable list\n",
        "\n",
        "### Behind the Scenes:\n",
        "\n",
        "```\n",
        "User query\n",
        "  ‚Üì\n",
        "query_a2a_agent(\"recent papers on transformer architectures 2023\")\n",
        "  ‚Üì\n",
        "A2A Agent ‚Üí ArXiv Tool ‚Üí ArXiv API\n",
        "  ‚Üì\n",
        "Returns: Paper metadata (titles, authors, abstracts)\n",
        "  ‚Üì\n",
        "LLM creates formatted list with key details\n",
        "```\n",
        "\n",
        "### Different Tool, Same Agent:\n",
        "The A2A agent internally decides to use ArXiv instead of web search based on the query!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üë§ User: Find recent papers on transformer architectures\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:__main__:Querying A2A: recent papers on transformer architectures 2023\n",
            "INFO:httpx:HTTP Request: POST http://localhost:10000/ \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Using: ['query_a2a_agent']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:a2a.client.client_task_manager:New task created with id: 4fa8feaa-809d-4fc7-ab23-174eab8bd737\n",
            "INFO:__main__:Got response: Here are some recent papers on transformer architectures published in 2023:\n",
            "\n",
            "1. **Title:** TurboViT:...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Result: Here are some recent papers on transformer architectures published in 2023:\n",
            "\n",
            "1. **Title:** TurboViT: Generating Fast Vision Transformers via Generativ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Agent: Here are some recent papers on transformer architectures published in 2023:\n",
            "\n",
            "1. **TurboViT: Generating Fast Vision Transformers via Generative Architecture Search**  \n",
            "   - **Authors:** Alexander Wong, Saad Abbasi, Saeejith Nair  \n",
            "   - **Published:** 2023-08-22  \n",
            "   - **Summary:** This paper explores the generation of fast vision transformer architectures using generative architecture search (GAS) to balance accuracy and computational efficiency. The TurboViT architecture is introduced, achieving significant reductions in architectural complexity and improved performance compared to existing models.\n",
            "\n",
            "2. **Transformers are Universal Predictors**  \n",
            "   - **Authors:** Sourya Basu, Moulik Choraria, Lav R. Varshney  \n",
            "   - **Published:** 2023-07-15  \n",
            "   - **Summary:** This study investigates the limits of the Transformer architecture for language modeling, demonstrating its universal prediction capabilities.\n",
            "\n",
            "3. **A survey of the Vision Transformers and their CNN-Transformer based Variants**  \n",
            "   - **Authors:** Asifullah Khan et al.  \n",
            "   - **Published:** 2024-07-27  \n",
            "   - **Summary:** This survey discusses the rise of vision transformers as alternatives to CNNs, focusing on hybrid architectures that combine convolutional operations with self-attention mechanisms to enhance performance in computer vision tasks.\n",
            "\n",
            "These papers reflect the ongoing advancements and research in transformer architectures across various applications.\n"
          ]
        }
      ],
      "source": [
        "await chat(\"Find recent papers on transformer architectures\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 3: Simple Query (No Tool Call)\n",
        "\n",
        "This test demonstrates **intelligent routing** - the agent answers directly without using tools.\n",
        "\n",
        "### What Should Happen:\n",
        "\n",
        "1. **User query**: \"What is 2 + 2?\"\n",
        "2. **LLM decision**: \"I know this answer ‚Üí no tools needed\"\n",
        "3. **Direct response**: \"2 + 2 equals 4.\"\n",
        "4. **No tool calls**: Notice no üîß or üìä symbols!\n",
        "\n",
        "### Why This Matters:\n",
        "\n",
        "This shows the agent is **truly autonomous**:\n",
        "- It doesn't blindly call tools for every query\n",
        "- It evaluates whether a tool is actually needed\n",
        "- It can answer directly when appropriate\n",
        "- This saves time and API costs\n",
        "\n",
        "### Decision Flow:\n",
        "\n",
        "```\n",
        "\"What is 2 + 2?\"\n",
        "  ‚Üì\n",
        "Agent analyzes: \"Simple math question\"\n",
        "  ‚Üì\n",
        "Decision: No external knowledge needed\n",
        "  ‚Üì\n",
        "Returns AIMessage with content=\"2 + 2 equals 4.\"\n",
        "  ‚Üì\n",
        "No tool_calls in message\n",
        "  ‚Üì\n",
        "should_continue() returns \"end\"\n",
        "  ‚Üì\n",
        "Skips tools node entirely\n",
        "```\n",
        "\n",
        "### Contrast with Test 1 & 2:\n",
        "- Test 1: LLM needed current info ‚Üí used tool\n",
        "- Test 2: LLM needed academic papers ‚Üí used tool  \n",
        "- Test 3: LLM has knowledge ‚Üí answered directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üë§ User: What is 2 + 2?\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Agent: 2 + 2 equals 4.\n"
          ]
        }
      ],
      "source": [
        "await chat(\"What is 2 + 2?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 4: Multi-Turn Conversation\n",
        "\n",
        "This test demonstrates **context maintenance** across multiple turns.\n",
        "\n",
        "### What Should Happen:\n",
        "\n",
        "**Turn 1:**\n",
        "1. User: \"Tell me about RAG\"\n",
        "2. Agent uses A2A to get RAG explanation\n",
        "3. Response stored in `thread_id=\"conversation1\"`\n",
        "\n",
        "**Turn 2:**\n",
        "1. User: \"Find papers on this topic\"\n",
        "2. Agent reads conversation history\n",
        "3. Understands \"this topic\" = RAG (from Turn 1!)\n",
        "4. Queries for RAG papers\n",
        "\n",
        "### How Context Works:\n",
        "\n",
        "```python\n",
        "# Turn 1 - stored in memory\n",
        "await chat(\"Tell me about RAG\", \"conversation1\")\n",
        "# Memory now contains:\n",
        "# - HumanMessage(\"Tell me about RAG\")\n",
        "# - AIMessage(tool_call to A2A)\n",
        "# - ToolMessage(RAG explanation)\n",
        "# - AIMessage(formatted explanation)\n",
        "\n",
        "# Turn 2 - reads from same memory\n",
        "await chat(\"Find papers on this topic\", \"conversation1\")\n",
        "# LLM sees ALL previous messages\n",
        "# Knows \"this topic\" refers to RAG\n",
        "```\n",
        "\n",
        "### Key Mechanism:\n",
        "\n",
        "1. **Same thread_id**: Both calls use `\"conversation1\"`\n",
        "2. **MemorySaver**: Persists messages between calls\n",
        "3. **LLM context**: Full conversation history sent to LLM\n",
        "4. **Reference resolution**: LLM resolves \"this topic\" ‚Üí \"RAG\"\n",
        "\n",
        "### What If We Used Different thread_ids?\n",
        "\n",
        "```python\n",
        "await chat(\"Tell me about RAG\", \"thread_A\")\n",
        "await chat(\"Find papers on this topic\", \"thread_B\")  # ‚ùå No context!\n",
        "```\n",
        "\n",
        "The second query would fail because there's no shared history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üë§ User: Tell me about RAG\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:__main__:Querying A2A: What is RAG (Retrieval-Augmented Generation)?\n",
            "INFO:httpx:HTTP Request: POST http://localhost:10000/ \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Using: ['query_a2a_agent']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:a2a.client.client_task_manager:New task created with id: adedea68-b7ae-433c-afb4-aced5cb2dc92\n",
            "INFO:__main__:Got response: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based methods with gener...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Result: Retrieval-Augmented Generation (RAG) is a framework that combines retrieval-based methods with generative models to enhance the performance of natural...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Agent: Retrieval-Augmented Generation (RAG) is a framework that enhances natural language processing tasks by combining retrieval-based methods with generative models. The main idea is to leverage external knowledge sources, such as large databases or documents, to improve the quality and relevance of generated responses.\n",
            "\n",
            "A typical RAG setup involves two main components:\n",
            "\n",
            "1. **Retrieval**: The model first retrieves relevant documents or information from a large corpus based on the input query. This can be done using techniques like dense retrieval or traditional keyword-based search.\n",
            "\n",
            "2. **Generation**: After retrieving the relevant information, a generative model (often a transformer-based model) uses this information to produce a coherent and contextually appropriate response.\n",
            "\n",
            "RAG models are particularly useful in scenarios where the knowledge required to answer a question is not contained within the model's training data, allowing them to access up-to-date or specialized information dynamically. This approach has been shown to improve performance on various tasks, including question answering, dialogue systems, and summarization.\n",
            "\n",
            "============================================================\n",
            "üë§ User: Find papers on this topic\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:__main__:Querying A2A: Retrieval-Augmented Generation RAG research papers\n",
            "INFO:httpx:HTTP Request: POST http://localhost:10000/ \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß Using: ['query_a2a_agent']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:a2a.client.client_task_manager:New task created with id: e6cc2fb2-a49c-43f2-9af0-5bc0512d4c5c\n",
            "INFO:__main__:Got response: Here are some recent research papers on Retrieval-Augmented Generation (RAG):\n",
            "\n",
            "1. **RAG-Stack: Co-Op...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Result: Here are some recent research papers on Retrieval-Augmented Generation (RAG):\n",
            "\n",
            "1. **RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vect...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Agent: Here are some recent research papers on Retrieval-Augmented Generation (RAG):\n",
            "\n",
            "1. **RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector Database Perspective**  \n",
            "   - **Authors:** Wenqi Jiang  \n",
            "   - **Published:** 2025-10-23  \n",
            "   - **Summary:** This paper discusses the integration of documents retrieved from a vector database into the prompts of large language models (LLMs) to enhance content generation. It presents RAG-Stack, a blueprint for optimizing both system performance and generation quality in RAG systems, comprising three components: RAG-IR, RAG-CM, and RAG-PE.\n",
            "\n",
            "2. **Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks**  \n",
            "   - **Authors:** Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang  \n",
            "   - **Published:** 2024-07-26  \n",
            "   - **Summary:** This paper introduces a modular framework for RAG systems, allowing for a more flexible and reconfigurable design. It decomposes complex RAG systems into independent modules and explores various RAG patterns, providing a roadmap for future developments in RAG technologies.\n",
            "\n",
            "3. **RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions**  \n",
            "   - **Authors:** Wanlong Liu, Junying Chen, Ke Ji, Li Zhou, Wenyu Chen, Benyou Wang  \n",
            "   - **Published:** 2024-12-31  \n",
            "   - **Summary:** This paper proposes RAG-Instruct, a method for synthesizing diverse and high-quality RAG instruction data to enhance LLMs. It addresses limitations in current RAG methods by leveraging multiple RAG paradigms and instruction simulation to improve task diversity.\n",
            "\n",
            "These papers explore various aspects of RAG, including optimization, modular design, and instruction enhancement.\n"
          ]
        }
      ],
      "source": [
        "await chat(\"Tell me about RAG\", \"conversation1\")\n",
        "await chat(\"Find papers on this topic\", \"conversation1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Always close the async HTTP client to free resources.\n",
        "\n",
        "### Why This Matters:\n",
        "\n",
        "- **Resource cleanup**: Closes TCP connections\n",
        "- **Best practice**: Prevents resource leaks\n",
        "- **Required for async**: Async contexts should be properly closed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done!\n"
          ]
        }
      ],
      "source": [
        "# Cleanup\n",
        "await httpx_client.aclose()\n",
        "print(\"‚úÖ Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Key Learnings\n",
        "\n",
        "### What This Notebook Demonstrated:\n",
        "\n",
        "1. **A2A Protocol Integration**\n",
        "   - Connecting to A2A-compliant agents\n",
        "   - Using AgentCards for service discovery\n",
        "   - Parsing A2A response structure (`payload.artifact.parts[0].root.text`)\n",
        "\n",
        "2. **LangGraph Agent Architecture**\n",
        "   - Building a graph with nodes (agent, tools) and edges (flow)\n",
        "   - Autonomous decision-making (when to use tools)\n",
        "   - State management with MessagesState\n",
        "\n",
        "3. **Async Execution Patterns**\n",
        "   - Using `async def` for async operations\n",
        "   - Using `await` for async calls\n",
        "   - Using `graph.astream()` instead of `graph.stream()`\n",
        "   - Using `async for` for streaming\n",
        "\n",
        "4. **Tool Abstraction**\n",
        "   - Converting Python functions to tools with `@tool`\n",
        "   - Automatic schema generation\n",
        "   - LLM-driven tool selection\n",
        "\n",
        "5. **Multi-Turn Context**\n",
        "   - Using `thread_id` to maintain conversations\n",
        "   - MemorySaver for persistence\n",
        "   - Reference resolution across turns\n",
        "\n",
        "### Key Technical Points:\n",
        "\n",
        "| Aspect | Implementation | Why It Matters |\n",
        "|--------|----------------|----------------|\n",
        "| **A2A Response** | `(task, payload)` tuples | Protocol standard |\n",
        "| **Text Location** | `payload.artifact.parts[0].root.text` | Where actual content lives |\n",
        "| **Async Tool** | `async def query_a2a_agent()` | Required for async client |\n",
        "| **Graph Execution** | `graph.astream()` | Works with async tools |\n",
        "| **Memory** | `MemorySaver()` + `thread_id` | Enables multi-turn |\n",
        "| **Routing** | `should_continue()` function | Controls flow |\n",
        "\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- [A2A Protocol Specification](https://github.com/missingstudio/a2a-protocol)\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [LangChain Tools Guide](https://python.langchain.com/docs/modules/tools/)\n",
        "- [httpx Async Client](https://www.python-httpx.org/async/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
