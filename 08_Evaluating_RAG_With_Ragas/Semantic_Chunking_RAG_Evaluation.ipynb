{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Chunking RAG Evaluation with RAGAS\n",
        "\n",
        "This notebook implements and evaluates a RAG application using semantic chunking strategies compared to naive retrieval.\n",
        "\n",
        "## Objectives:\n",
        "- Baseline LangGraph RAG Application using NAIVE RETRIEVAL\n",
        "- Baseline Evaluation using RAGAS METRICS\n",
        "- Implement SEMANTIC CHUNKING STRATEGY\n",
        "- Create LangGraph RAG Application using SEMANTIC CHUNKING with NAIVE RETRIEVAL\n",
        "- Compare and contrast results\n",
        "\n",
        "## RAGAS Metrics:\n",
        "- Faithfulness\n",
        "- Answer Relevancy\n",
        "- Context Precision\n",
        "- Context Recall\n",
        "- Answer Correctness\n",
        "\n",
        "## Our Semantic Chunking Strategy\n",
        "\n",
        "We implement a semantic chunking strategy that groups semantically similar sentences together while strictly enforcing size constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependencies loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_core.documents import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# RAGAS imports\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    AnswerRelevancy,\n",
        "    ContextPrecision,\n",
        "    FactualCorrectness, \n",
        "    LLMContextRecall\n",
        ")\n",
        "from ragas import evaluate, RunConfig\n",
        "from ragas.dataset_schema import EvaluationDataset\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "print(\"Dependencies loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# Set up API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key!\")\n",
        "\n",
        "# Initialize LLM and embeddings\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n",
        "embeddings = OpenAIEmbeddings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 64 documents\n",
            "Total characters: 112856\n",
            "Document 1: 1672 characters\n",
            "Document 2: 2051 characters\n",
            "Document 3: 3823 characters\n",
            "Document 4: 4164 characters\n",
            "Document 5: 3965 characters\n",
            "Document 6: 3635 characters\n",
            "Document 7: 2858 characters\n",
            "Document 8: 2895 characters\n",
            "Document 9: 2323 characters\n",
            "Document 10: 2940 characters\n",
            "Document 11: 2303 characters\n",
            "Document 12: 2195 characters\n",
            "Document 13: 1669 characters\n",
            "Document 14: 1523 characters\n",
            "Document 15: 1440 characters\n",
            "Document 16: 2425 characters\n",
            "Document 17: 863 characters\n",
            "Document 18: 2086 characters\n",
            "Document 19: 3097 characters\n",
            "Document 20: 1205 characters\n",
            "Document 21: 2087 characters\n",
            "Document 22: 2144 characters\n",
            "Document 23: 453 characters\n",
            "Document 24: 608 characters\n",
            "Document 25: 1888 characters\n",
            "Document 26: 474 characters\n",
            "Document 27: 3051 characters\n",
            "Document 28: 982 characters\n",
            "Document 29: 2180 characters\n",
            "Document 30: 2192 characters\n",
            "Document 31: 117 characters\n",
            "Document 32: 1228 characters\n",
            "Document 33: 3898 characters\n",
            "Document 34: 1255 characters\n",
            "Document 35: 95 characters\n",
            "Document 36: 559 characters\n",
            "Document 37: 1704 characters\n",
            "Document 38: 3420 characters\n",
            "Document 39: 2485 characters\n",
            "Document 40: 2679 characters\n",
            "Document 41: 2169 characters\n",
            "Document 42: 1031 characters\n",
            "Document 43: 2079 characters\n",
            "Document 44: 1620 characters\n",
            "Document 45: 1706 characters\n",
            "Document 46: 1431 characters\n",
            "Document 47: 1384 characters\n",
            "Document 48: 1198 characters\n",
            "Document 49: 1261 characters\n",
            "Document 50: 1393 characters\n",
            "Document 51: 1781 characters\n",
            "Document 52: 1309 characters\n",
            "Document 53: 462 characters\n",
            "Document 54: 2594 characters\n",
            "Document 55: 2404 characters\n",
            "Document 56: 135 characters\n",
            "Document 57: 146 characters\n",
            "Document 58: 1976 characters\n",
            "Document 59: 143 characters\n",
            "Document 60: 956 characters\n",
            "Document 61: 633 characters\n",
            "Document 62: 495 characters\n",
            "Document 63: 440 characters\n",
            "Document 64: 1479 characters\n"
          ]
        }
      ],
      "source": [
        "# Load documents\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents\")\n",
        "print(f\"Total characters: {sum(len(doc.page_content) for doc in docs)}\")\n",
        "\n",
        "# Display document info\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"Document {i+1}: {len(doc.page_content)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline RAG Application (Naive Retrieval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline chunks created: 275\n",
            "Baseline chunk size stats:\n",
            "  Min: 2\n",
            "  Max: 499\n",
            "  Mean: 409.60\n",
            "  Median: 448.00\n",
            "\n",
            "ðŸ“ˆ Character Distribution:\n",
            "Chunk 1: 474 characters\n",
            "Chunk 2: 485 characters\n",
            "Chunk 3: 486 characters\n",
            "Chunk 4: 222 characters\n",
            "Chunk 5: 473 characters\n",
            "... and 270 more chunks\n"
          ]
        }
      ],
      "source": [
        "# Create baseline chunks using RecursiveCharacterTextSplitter\n",
        "baseline_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, chunk_overlap=0,\n",
        ")\n",
        "\n",
        "baseline_chunks = baseline_splitter.split_documents(docs)\n",
        "print(f\"Baseline chunks created: {len(baseline_chunks)}\")\n",
        "\n",
        "baseline_chunk_sizes = [len(chunk.page_content) for chunk in baseline_chunks]\n",
        "print(f\"Baseline chunk size stats:\")\n",
        "print(f\"  Min: {min(baseline_chunk_sizes)}\")\n",
        "print(f\"  Max: {max(baseline_chunk_sizes)}\")\n",
        "print(f\"  Mean: {np.mean(baseline_chunk_sizes):.2f}\")\n",
        "print(f\"  Median: {np.median(baseline_chunk_sizes):.2f}\")\n",
        "\n",
        "# Show distribution\n",
        "print(f\"\\nðŸ“ˆ Character Distribution:\")\n",
        "for i, chunk in enumerate(baseline_chunks[:5]):  # Show first 5 chunks\n",
        "    print(f\"Chunk {i+1}: {len(chunk.page_content):,} characters\")\n",
        "if len(baseline_chunks) > 5:\n",
        "    print(f\"... and {len(baseline_chunks)-5} more chunks\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline vector store and retriever created!\n"
          ]
        }
      ],
      "source": [
        "# Create vector store for baseline\n",
        "baseline_vector_store = Qdrant.from_documents(\n",
        "    documents=baseline_chunks,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"baseline_rag\"\n",
        ")\n",
        "\n",
        "baseline_retriever = baseline_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "print(\"Baseline vector store and retriever created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline RAG application created successfully!\n"
          ]
        }
      ],
      "source": [
        "class BaselineState(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    response: str\n",
        "\n",
        "# Define retrieval function\n",
        "def baseline_retrieve(state: BaselineState) -> Dict[str, List[Document]]:\n",
        "    \"\"\"Retrieve relevant documents for the question\"\"\"\n",
        "    retrieved_docs = baseline_retriever.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "# Define RAG prompt\n",
        "RAG_PROMPT = \"\"\"\n",
        "You are a helpful assistant who answers questions based on provided context. \n",
        "You must only use the provided context, and cannot use your own knowledge.\n",
        "\n",
        "### Question\n",
        "{question}\n",
        "\n",
        "### Context\n",
        "{context}\n",
        "\n",
        "Please provide a comprehensive answer based on the context above.\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
        "\n",
        "# Define generation function\n",
        "def baseline_generate(state: BaselineState) -> Dict[str, Any]:\n",
        "    \"\"\"Generate response based on retrieved context\"\"\"\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"response\": response.content}\n",
        "\n",
        "# Build baseline RAG graph\n",
        "baseline_graph_builder = StateGraph(BaselineState).add_sequence([baseline_retrieve, baseline_generate])\n",
        "baseline_graph_builder.add_edge(START, \"baseline_retrieve\")\n",
        "baseline_graph = baseline_graph_builder.compile()\n",
        "\n",
        "print(\"Baseline RAG application created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Semantic Chunking Implementation\n",
        "\n",
        "We implement a straightforward function for semantic chunking that meets your exact requirements:\n",
        "\n",
        "\n",
        "### **Core Algorithm:**\n",
        "1. **Sentence Embedding**: Generate embeddings for all sentences using OpenAI embeddings\n",
        "2. **Semantic Grouping**: Group sentences using a greedy algorithm that enforces similarity measure (cosine distance)\n",
        "3. **Size Enforcement**: Apply strict size limits with multi-level fallback strategy\n",
        "\n",
        "### **Multi-Level Fallback Strategy:**\n",
        "- **Level 1**: Semantic grouping based on sentence similarity\n",
        "- **Level 2**: Paragraph splitting when groups exceed `max_chunk_size`\n",
        "- **Level 3**: Sentence splitting when paragraphs exceed `max_chunk_size`\n",
        "- **Level 4**: Word splitting when sentences exceed `max_chunk_size`\n",
        "\n",
        "### **Key Features:**\n",
        "- **Semantic Coherence**: Groups related sentences together for better context\n",
        "- **Size Compliance**: Guarantees no chunk exceeds specified limits\n",
        "- **Flexible Thresholds**: Configurable similarity threshold and size limits\n",
        "- **Metadata Tracking**: Comprehensive metadata for evaluation and debugging\n",
        "- **Production Ready**: Robust error handling and edge case management\n",
        "\n",
        "### **Benefits Over Naive Chunking:**\n",
        "- Better semantic coherence in retrieved contexts\n",
        "- Reduced chunk fragmentation\n",
        "- Improved answer quality through better context grouping\n",
        "- Maintains semantic relationships while respecting size constraints \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def semantic_chunk_documents(\n",
        "    documents: List[Document], \n",
        "    embeddings_model: OpenAIEmbeddings,\n",
        "    similarity_threshold: float = 0.7,\n",
        "    max_chunk_size: int = 1000,\n",
        "    min_chunk_size: int = 50\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Simple function for semantic chunking that meets requirements:\n",
        "    - Chunk semantically similar sentences (based on threshold)\n",
        "    - Then paragraphs, greedily\n",
        "    - Up to a maximum chunk size (strictly enforced)\n",
        "    - Minimum chunk size is a single sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    def preprocess_text(text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        return re.sub(r'\\s+', ' ', text.strip())\n",
        "    \n",
        "    def get_sentence_embeddings(sentences: List[str]) -> np.ndarray:\n",
        "        \"\"\"Get embeddings for sentences\"\"\"\n",
        "        embeddings = embeddings_model.embed_documents(sentences)\n",
        "        return np.array(embeddings)\n",
        "    \n",
        "    def group_similar_sentences(sentences: List[str], embeddings: np.ndarray, similarity_threshold: float) -> List[List[int]]:\n",
        "        \"\"\"Group semantically similar sentences greedily\"\"\"\n",
        "        similarity_matrix = cosine_similarity(embeddings)\n",
        "        groups = []\n",
        "        used_indices = set()\n",
        "        \n",
        "        for i in range(len(sentences)):\n",
        "            if i in used_indices:\n",
        "                continue\n",
        "                \n",
        "            group = [i]\n",
        "            used_indices.add(i)\n",
        "            \n",
        "            # Greedily find similar sentences\n",
        "            for j in range(i + 1, len(sentences)):\n",
        "                if j in used_indices:\n",
        "                    continue\n",
        "                    \n",
        "                if similarity_matrix[i][j] >= similarity_threshold:\n",
        "                    group.append(j)\n",
        "                    used_indices.add(j)\n",
        "            \n",
        "            groups.append(group)\n",
        "        \n",
        "        return groups\n",
        "    \n",
        "    def handle_single_sentence(text: str, doc: Document, max_chunk_size: int) -> List[Document]:\n",
        "        \"\"\"Handle documents with single sentences, enforcing max_chunk_size\"\"\"\n",
        "        chunks = []\n",
        "        \n",
        "        if len(text) > max_chunk_size:\n",
        "            # Split the single sentence by words if it's too long\n",
        "            words = text.split()\n",
        "            current_chunk = \"\"\n",
        "            chunk_id = 0\n",
        "            \n",
        "            for word in words:\n",
        "                if len(current_chunk + word) > max_chunk_size:\n",
        "                    if current_chunk.strip():\n",
        "                        chunk_doc = Document(page_content=current_chunk.strip(), metadata=doc.metadata.copy())\n",
        "                        chunk_doc.metadata['chunking_method'] = 'semantic'\n",
        "                        chunk_doc.metadata['chunk_id'] = chunk_id\n",
        "                        chunk_doc.metadata['chunk_size'] = len(current_chunk.strip())\n",
        "                        chunk_doc.metadata['config'] = {\n",
        "                            'similarity_threshold': similarity_threshold,\n",
        "                            'max_chunk_size': max_chunk_size,\n",
        "                            'min_chunk_size': min_chunk_size\n",
        "                        }\n",
        "                        chunks.append(chunk_doc)\n",
        "                        chunk_id += 1\n",
        "                    current_chunk = word + \" \"\n",
        "                else:\n",
        "                    current_chunk += word + \" \"\n",
        "            \n",
        "            if current_chunk.strip():\n",
        "                chunk_doc = Document(page_content=current_chunk.strip(), metadata=doc.metadata.copy())\n",
        "                chunk_doc.metadata['chunking_method'] = 'semantic'\n",
        "                chunk_doc.metadata['chunk_id'] = chunk_id\n",
        "                chunk_doc.metadata['chunk_size'] = len(current_chunk.strip())\n",
        "                chunk_doc.metadata['config'] = {\n",
        "                    'similarity_threshold': similarity_threshold,\n",
        "                    'max_chunk_size': max_chunk_size,\n",
        "                    'min_chunk_size': min_chunk_size\n",
        "                }\n",
        "                chunks.append(chunk_doc)\n",
        "        else:\n",
        "            # Single sentence within limits\n",
        "            chunk_doc = Document(page_content=text, metadata=doc.metadata.copy())\n",
        "            chunk_doc.metadata['chunking_method'] = 'semantic'\n",
        "            chunk_doc.metadata['chunk_id'] = 0\n",
        "            chunk_doc.metadata['chunk_size'] = len(text)\n",
        "            chunk_doc.metadata['config'] = {\n",
        "                'similarity_threshold': similarity_threshold,\n",
        "                'max_chunk_size': max_chunk_size,\n",
        "                'min_chunk_size': min_chunk_size\n",
        "            }\n",
        "            chunks.append(chunk_doc)\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def create_chunks_from_groups(sentences: List[str], groups: List[List[int]]) -> List[str]:\n",
        "        \"\"\"Create chunks from grouped sentences, strictly enforcing max chunk size\"\"\"\n",
        "        chunks = []\n",
        "        \n",
        "        for group in groups:\n",
        "            # Sort group indices to maintain order\n",
        "            group.sort()\n",
        "            group_sentences = [sentences[i] for i in group]\n",
        "            chunk_text = ' '.join(group_sentences)\n",
        "            \n",
        "            # Strictly enforce max_chunk_size\n",
        "            if len(chunk_text) > max_chunk_size:\n",
        "                # Split by paragraphs first\n",
        "                paragraphs = chunk_text.split('\\n\\n')\n",
        "                current_chunk = \"\"\n",
        "                \n",
        "                for paragraph in paragraphs:\n",
        "                    # If single paragraph exceeds limit, split it by sentences\n",
        "                    if len(paragraph) > max_chunk_size:\n",
        "                        # Save current chunk if it exists\n",
        "                        if current_chunk.strip():\n",
        "                            chunks.append(current_chunk.strip())\n",
        "                            current_chunk = \"\"\n",
        "                        \n",
        "                        # Split paragraph into sentences and add them individually\n",
        "                        para_sentences = sent_tokenize(paragraph)\n",
        "                        for sentence in para_sentences:\n",
        "                            # If single sentence exceeds limit, split by words\n",
        "                            if len(sentence) > max_chunk_size:\n",
        "                                if current_chunk.strip():\n",
        "                                    chunks.append(current_chunk.strip())\n",
        "                                    current_chunk = \"\"\n",
        "                                \n",
        "                                # Split sentence by words\n",
        "                                words = sentence.split()\n",
        "                                temp_chunk = \"\"\n",
        "                                for word in words:\n",
        "                                    if len(temp_chunk + word) > max_chunk_size:\n",
        "                                        if temp_chunk.strip():\n",
        "                                            chunks.append(temp_chunk.strip())\n",
        "                                        temp_chunk = word + \" \"\n",
        "                                    else:\n",
        "                                        temp_chunk += word + \" \"\n",
        "                                \n",
        "                                if temp_chunk.strip():\n",
        "                                    current_chunk = temp_chunk.strip()\n",
        "                            else:\n",
        "                                # Check if adding this sentence would exceed limit\n",
        "                                if len(current_chunk + \" \" + sentence) > max_chunk_size:\n",
        "                                    if current_chunk.strip():\n",
        "                                        chunks.append(current_chunk.strip())\n",
        "                                    current_chunk = sentence\n",
        "                                else:\n",
        "                                    current_chunk += (\" \" + sentence) if current_chunk else sentence\n",
        "                    else:\n",
        "                        # Check if adding this paragraph would exceed limit\n",
        "                        if len(current_chunk + \"\\n\\n\" + paragraph) > max_chunk_size:\n",
        "                            if current_chunk.strip():\n",
        "                                chunks.append(current_chunk.strip())\n",
        "                            current_chunk = paragraph\n",
        "                        else:\n",
        "                            current_chunk += (\"\\n\\n\" + paragraph) if current_chunk else paragraph\n",
        "                \n",
        "                if current_chunk.strip():\n",
        "                    chunks.append(current_chunk.strip())\n",
        "            else:\n",
        "                chunks.append(chunk_text)\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    # Main processing\n",
        "    all_chunks = []\n",
        "    \n",
        "    for doc in documents:\n",
        "        text = preprocess_text(doc.page_content)\n",
        "        \n",
        "        # Split into sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "        \n",
        "        if len(sentences) <= 1:\n",
        "            single_sentence_chunks = handle_single_sentence(text, doc, max_chunk_size)\n",
        "            all_chunks.extend(single_sentence_chunks)\n",
        "            continue\n",
        "        \n",
        "        # Get embeddings for sentences\n",
        "        sentence_embeddings = get_sentence_embeddings(sentences)\n",
        "        \n",
        "        # Group similar sentences\n",
        "        groups = group_similar_sentences(sentences, sentence_embeddings, similarity_threshold)\n",
        "        \n",
        "        # Create chunks from groups while strictly enforcing max_chunk_size\n",
        "        chunks = create_chunks_from_groups(sentences, groups)\n",
        "        \n",
        "        # Filter chunks by minimum size (single sentence minimum)\n",
        "        filtered_chunks = [chunk for chunk in chunks if len(chunk) >= min_chunk_size]\n",
        "        \n",
        "        # Create Document objects\n",
        "        for i, chunk in enumerate(filtered_chunks):\n",
        "            metadata = doc.metadata.copy()\n",
        "            metadata['chunk_id'] = i\n",
        "            metadata['chunking_method'] = 'semantic'\n",
        "            metadata['chunk_size'] = len(chunk)\n",
        "            metadata['config'] = {\n",
        "                'similarity_threshold': similarity_threshold,\n",
        "                'max_chunk_size': max_chunk_size,\n",
        "                'min_chunk_size': min_chunk_size\n",
        "            }\n",
        "            all_chunks.append(Document(page_content=chunk, metadata=metadata))\n",
        "    \n",
        "    return all_chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic chunks created: 355\n",
            "Semantic chunk size stats:\n",
            "  Min: 51\n",
            "  Max: 699\n",
            "  Mean: 311.97\n",
            "  Median: 253.00\n",
            "\n",
            "ðŸ“ˆ Character Distribution:\n",
            "Chunk 1: 693 characters\n",
            "Chunk 2: 273 characters\n",
            "Chunk 3: 107 characters\n",
            "Chunk 4: 156 characters\n",
            "Chunk 5: 99 characters\n",
            "... and 350 more chunks\n"
          ]
        }
      ],
      "source": [
        "# Create semantic chunks \n",
        "semantic_chunks = semantic_chunk_documents(\n",
        "    documents=docs,\n",
        "    embeddings_model=embeddings,\n",
        "    similarity_threshold=0.8,\n",
        "    max_chunk_size=700,\n",
        "    min_chunk_size=50\n",
        ")\n",
        "print(f\"Semantic chunks created: {len(semantic_chunks)}\")\n",
        "\n",
        "# Display chunk statistics\n",
        "chunk_sizes = [len(chunk.page_content) for chunk in semantic_chunks]\n",
        "print(f\"Semantic chunk size stats:\")\n",
        "print(f\"  Min: {min(chunk_sizes)}\")\n",
        "print(f\"  Max: {max(chunk_sizes)}\")\n",
        "print(f\"  Mean: {np.mean(chunk_sizes):.2f}\")\n",
        "print(f\"  Median: {np.median(chunk_sizes):.2f}\")\n",
        "\n",
        "\n",
        "# Show distribution\n",
        "print(f\"\\nðŸ“ˆ Character Distribution:\")\n",
        "for i, chunk in enumerate(semantic_chunks[:5]):  # Show first 5 chunks\n",
        "    print(f\"Chunk {i+1}: {len(chunk.page_content):,} characters\")\n",
        "if len(semantic_chunks) > 5:\n",
        "    print(f\"... and {len(semantic_chunks)-5} more chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'At least one co-author has disclosed additional relationships of potential relevance for this research. \\nFurther information is available online at http://www.nber.org/papers/w34255\\nNBER working papers are circulated for discussion and comment purposes. They have not been \\npeer-reviewed or been subject to the review by the NBER Board of Directors that accompanies \\nofficial NBER publications.\\nÂ© 2025 by Aaron Chatterji, Thomas Cunningham, David J. Deming, Zoe Hitzig, Christopher Ong,'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_chunks[2].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'We especially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for this work.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_chunks[2].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic vector store and retriever created!\n"
          ]
        }
      ],
      "source": [
        "# Create vector store for semantic chunks\n",
        "semantic_vector_store = Qdrant.from_documents(\n",
        "    documents=semantic_chunks,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"semantic_rag\"\n",
        ")\n",
        "\n",
        "semantic_retriever = semantic_vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"Semantic vector store and retriever created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic RAG application created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define LangGraph state for semantic RAG\n",
        "class SemanticState(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    response: str\n",
        "\n",
        "# Define retrieval function for semantic RAG\n",
        "def semantic_retrieve(state: SemanticState) -> Dict[str, List[Document]]:\n",
        "    \"\"\"Retrieve relevant documents for the question using semantic chunks\"\"\"\n",
        "    retrieved_docs = semantic_retriever.invoke(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "# Define generation function for semantic RAG\n",
        "def semantic_generate(state: SemanticState) -> Dict[str, Any]:\n",
        "    \"\"\"Generate response based on retrieved semantic context\"\"\"\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"response\": response.content}\n",
        "\n",
        "# Build semantic RAG graph\n",
        "semantic_graph_builder = StateGraph(SemanticState).add_sequence([semantic_retrieve, semantic_generate])\n",
        "semantic_graph_builder.add_edge(START, \"semantic_retrieve\")\n",
        "semantic_graph = semantic_graph_builder.compile()\n",
        "\n",
        "print(\"Semantic RAG application created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Synthetic Data for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the same model configuration as the working notebook\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41aebf299a1b4523928c3ddd1b28093e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f424e0254f546899b3632714bc763cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/64 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n",
            "unable to apply transformation: 'headlines' property not found in this node\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "794b1f301b874dc9a1e3369656c401c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node 'aac21c'. Skipping!\n",
            "Property 'summary' already exists in node 'a061dc'. Skipping!\n",
            "Property 'summary' already exists in node 'b9786e'. Skipping!\n",
            "Property 'summary' already exists in node 'f69887'. Skipping!\n",
            "Property 'summary' already exists in node 'e0a420'. Skipping!\n",
            "Property 'summary' already exists in node '734e07'. Skipping!\n",
            "Property 'summary' already exists in node '92f509'. Skipping!\n",
            "Property 'summary' already exists in node 'b16d37'. Skipping!\n",
            "Property 'summary' already exists in node '95b87a'. Skipping!\n",
            "Property 'summary' already exists in node '536e13'. Skipping!\n",
            "Property 'summary' already exists in node '48652a'. Skipping!\n",
            "Property 'summary' already exists in node 'c4fee6'. Skipping!\n",
            "Property 'summary' already exists in node '232502'. Skipping!\n",
            "Property 'summary' already exists in node 'a0e5d6'. Skipping!\n",
            "Property 'summary' already exists in node 'dc37ec'. Skipping!\n",
            "Property 'summary' already exists in node '150b46'. Skipping!\n",
            "Property 'summary' already exists in node '8bd3e0'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4f889423b024088bdf40660350e2838",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd2fdec4be8b4e7984274f6b3fcd75f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node '8bd3e0'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'dc37ec'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '150b46'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '48652a'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'a061dc'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '95b87a'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '232502'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'aac21c'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'e0a420'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'f69887'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'b9786e'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '92f509'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'a0e5d6'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '734e07'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'b16d37'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'c4fee6'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '536e13'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4339f09118194164893c2943a14d1459",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34298bd98088483b8ad5c8573bee35db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5a0d18696ba458e93f98aec169a1e26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d75a39258f743a4a488951d919c10bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generator = TestsetGenerator(llm=evaluator_llm, embedding_model=evaluator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)\n",
        "\n",
        "\n",
        "\n",
        "# # Load existing test data\n",
        "# import ast \n",
        "\n",
        "# test_data = pd.read_csv(\"test_data_RAG_nb1.csv\")\n",
        "# print(f\"Loaded {len(test_data)} test samples from CSV\")\n",
        "\n",
        "# # Fix the reference_contexts column - convert string representations to actual lists\n",
        "# test_data['reference_contexts'] = test_data['reference_contexts'].apply(\n",
        "#     lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "# )\n",
        "\n",
        "# print(\"âœ… Fixed reference_contexts format\")\n",
        "\n",
        "# # Convert to RAGAS dataset format\n",
        "# dataset = EvaluationDataset.from_pandas(test_data)\n",
        "# print(\"âœ… Using existing test data instead of generating new one\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What does Acemoglu (2024) contribute to the di...</td>\n",
              "      <td>[Introduction ChatGPT launched in November 202...</td>\n",
              "      <td>The context notes that the sudden growth in la...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Could you provide a detailed breakdown of the ...</td>\n",
              "      <td>[Month Non-Work (M) (%) Work (M) (%) Total Mes...</td>\n",
              "      <td>In Jun 2024, the total number of messages was ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What were the key trends in ChatGPT message us...</td>\n",
              "      <td>[Table 1: ChatGPT daily message counts (millio...</td>\n",
              "      <td>Leading up to the 26th of June 2024, ChatGPT e...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chatgpt business is what?</td>\n",
              "      <td>[Variation by Occupation Figure 23 presents va...</td>\n",
              "      <td>ChatGPT Business (formerly known as Teams) is ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does the role of ChatGPT as an advisor and...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nConclusion This paper studies the ...</td>\n",
              "      <td>The role of ChatGPT as an advisor and decision...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>how chatgpt get so many users so fast and what...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
              "      <td>chatgpt launched in november 2022 and by july ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does the rapid adoption and diffusion of C...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
              "      <td>The rapid adoption and diffusion of ChatGPT, a...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Based on the provided message volume statistic...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nMonth Non-Work (M) (%) Work (M) (%...</td>\n",
              "      <td>Between June 2024 and June 2025, the total dai...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>By July 2025, how did the rapid growth of Chat...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
              "      <td>By July 2025, ChatGPT had achieved unprecedent...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What was the total number of messages sent on ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
              "      <td>In June 2025, a total of 2,627 million (2.627 ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>According to data from 2024, what were the mai...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
              "      <td>In 2024, the main categories of ChatGPT usage ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How did the proportion and volume of non-work ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nConclusion This paper studies the ...</td>\n",
              "      <td>Between June 2024 and June 2025, the volume of...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What does Acemoglu (2024) contribute to the di...   \n",
              "1   Could you provide a detailed breakdown of the ...   \n",
              "2   What were the key trends in ChatGPT message us...   \n",
              "3                           chatgpt business is what?   \n",
              "4   How does the role of ChatGPT as an advisor and...   \n",
              "5   how chatgpt get so many users so fast and what...   \n",
              "6   How does the rapid adoption and diffusion of C...   \n",
              "7   Based on the provided message volume statistic...   \n",
              "8   By July 2025, how did the rapid growth of Chat...   \n",
              "9   What was the total number of messages sent on ...   \n",
              "10  According to data from 2024, what were the mai...   \n",
              "11  How did the proportion and volume of non-work ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [Introduction ChatGPT launched in November 202...   \n",
              "1   [Month Non-Work (M) (%) Work (M) (%) Total Mes...   \n",
              "2   [Table 1: ChatGPT daily message counts (millio...   \n",
              "3   [Variation by Occupation Figure 23 presents va...   \n",
              "4   [<1-hop>\\n\\nConclusion This paper studies the ...   \n",
              "5   [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
              "6   [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
              "7   [<1-hop>\\n\\nMonth Non-Work (M) (%) Work (M) (%...   \n",
              "8   [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
              "9   [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
              "10  [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
              "11  [<1-hop>\\n\\nConclusion This paper studies the ...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   The context notes that the sudden growth in la...   \n",
              "1   In Jun 2024, the total number of messages was ...   \n",
              "2   Leading up to the 26th of June 2024, ChatGPT e...   \n",
              "3   ChatGPT Business (formerly known as Teams) is ...   \n",
              "4   The role of ChatGPT as an advisor and decision...   \n",
              "5   chatgpt launched in november 2022 and by july ...   \n",
              "6   The rapid adoption and diffusion of ChatGPT, a...   \n",
              "7   Between June 2024 and June 2025, the total dai...   \n",
              "8   By July 2025, ChatGPT had achieved unprecedent...   \n",
              "9   In June 2025, a total of 2,627 million (2.627 ...   \n",
              "10  In 2024, the main categories of ChatGPT usage ...   \n",
              "11  Between June 2024 and June 2025, the volume of...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_df = dataset.to_pandas()\n",
        "\n",
        "# save evaluation dataset df\n",
        "dataset_df.to_csv(\"evaluation_dataset_AB.csv\", index=False)\n",
        "\n",
        "dataset_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Evaluation Comparison between RAG systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = [LLMContextRecall(), ContextPrecision(), Faithfulness(), FactualCorrectness(), AnswerRelevancy()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Running Baseline RAG system on test dataset...\n",
            "âœ… Baseline RAG responses generated!\n"
          ]
        }
      ],
      "source": [
        "# Step 1a: Generate responses from Baseline RAG system\n",
        "import time\n",
        "import copy\n",
        "\n",
        "print(\"ðŸ”„ Running Baseline RAG system on test dataset...\")\n",
        "for test_row in dataset:\n",
        "    response = baseline_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
        "    test_row.eval_sample.response = response[\"response\"]\n",
        "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "print(\"âœ… Baseline RAG responses generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Evaluating Baseline RAG system...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f6790ee868043e5bcbf51cf7949d862",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-GqxpYrKHFaUcOMjEhQsI0XlV on tokens per min (TPM): Limit 30000, Used 29490, Requested 1079. Please try again in 1.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[22]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-GqxpYrKHFaUcOMjEhQsI0XlV on tokens per min (TPM): Limit 30000, Used 30000, Requested 1812. Please try again in 3.624s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[26]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-GqxpYrKHFaUcOMjEhQsI0XlV on tokens per min (TPM): Limit 30000, Used 30000, Requested 1827. Please try again in 3.654s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
            "Exception raised in Job[16]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-GqxpYrKHFaUcOMjEhQsI0XlV on tokens per min (TPM): Limit 30000, Used 30000, Requested 1291. Please try again in 2.582s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Baseline evaluation completed!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.4722, 'faithfulness': 0.7648, 'factual_correctness': 0.5755, 'answer_relevancy': 0.7900, 'context_precision': 0.5909}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1b: Evaluate Baseline RAG system\n",
        "print(\"ðŸ”„ Evaluating Baseline RAG system...\")\n",
        "\n",
        "baseline_evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "\n",
        "baseline_result = evaluate(\n",
        "    dataset=baseline_evaluation_dataset,\n",
        "    metrics=metrics,\n",
        "    llm=evaluator_llm,\n",
        "    run_config=RunConfig(timeout=360)\n",
        ")\n",
        "\n",
        "print(\"âœ… Baseline evaluation completed!\")\n",
        "baseline_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Running Semantic RAG system on test dataset...\n",
            "âœ… Semantic RAG responses generated!\n"
          ]
        }
      ],
      "source": [
        "# Step 2a: Generate responses from Semantic RAG system\n",
        "semantic_dataset = copy.deepcopy(dataset)\n",
        "\n",
        "print(\"ðŸ”„ Running Semantic RAG system on test dataset...\")\n",
        "for test_row in semantic_dataset:\n",
        "    response = semantic_graph.invoke({\"question\": test_row.eval_sample.user_input})\n",
        "    test_row.eval_sample.response = response[\"response\"]\n",
        "    test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "\n",
        "print(\"âœ… Semantic RAG responses generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Evaluating Semantic RAG system...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72d25b9d9aba48d889ad55b523d908a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Semantic evaluation completed!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.7639, 'faithfulness': 0.8815, 'factual_correctness': 0.5933, 'answer_relevancy': 0.9555, 'context_precision': 0.6906}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2b: Evaluate Semantic RAG system\n",
        "print(\"ðŸ”„ Evaluating Semantic RAG system...\")\n",
        "\n",
        "semantic_evaluation_dataset = EvaluationDataset.from_pandas(semantic_dataset.to_pandas())\n",
        "\n",
        "semantic_result = evaluate(\n",
        "    dataset=semantic_evaluation_dataset,\n",
        "    metrics=metrics,\n",
        "    llm=evaluator_llm,\n",
        "    run_config=RunConfig(timeout=600, max_retries=3, max_workers=4, max_wait=60)\n",
        ")\n",
        "\n",
        "print(\"âœ… Semantic evaluation completed!\")\n",
        "semantic_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Comprehensive Evaluation Comparison\n",
        "\n",
        "This section provides a detailed comparison between the **Baseline RAG** (using RecursiveCharacterTextSplitter) and the **Semantic RAG** (using semantic chunking) systems across key RAGAS metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Baseline RAG Scores:\n",
            "  context_recall: 0.4722\n",
            "  context_precision: 0.5909\n",
            "  faithfulness: 0.7648\n",
            "  factual_correctness: 0.5755\n",
            "  answer_relevancy: 0.7900\n",
            "\n",
            "ðŸ“Š Semantic RAG Scores:\n",
            "  context_recall: 0.7639\n",
            "  context_precision: 0.6906\n",
            "  faithfulness: 0.8815\n",
            "  factual_correctness: 0.5933\n",
            "  answer_relevancy: 0.9555\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Extract metric scores from results\n",
        "baseline_scores = {\n",
        "    \"context_recall\": baseline_result['context_recall'],\n",
        "    \"context_precision\": baseline_result['context_precision'],\n",
        "    \"faithfulness\": baseline_result['faithfulness'],\n",
        "    \"factual_correctness\": baseline_result['factual_correctness'],\n",
        "    \"answer_relevancy\": baseline_result['answer_relevancy']\n",
        "}\n",
        "\n",
        "semantic_scores = {\n",
        "    \"context_recall\": semantic_result['context_recall'],\n",
        "    \"context_precision\": semantic_result['context_precision'],\n",
        "    \"faithfulness\": semantic_result['faithfulness'],\n",
        "    \"factual_correctness\": semantic_result['factual_correctness'],\n",
        "    \"answer_relevancy\": semantic_result['answer_relevancy']\n",
        "}\n",
        "\n",
        "print(\"ðŸ“Š Baseline RAG Scores:\")\n",
        "for metric, scores in baseline_scores.items():\n",
        "    print(f\"  {metric}: {np.nanmean(scores):.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Semantic RAG Scores:\")\n",
        "for metric, scores in semantic_scores.items():\n",
        "    print(f\"  {metric}: {np.nanmean(scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Comparison</th>\n",
              "      <th>Stage</th>\n",
              "      <th>What it Measures</th>\n",
              "      <th>Better</th>\n",
              "      <th>Baseline Score</th>\n",
              "      <th>Semantic Score</th>\n",
              "      <th>Improvement</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Context Precision</td>\n",
              "      <td>Question â†’ Context</td>\n",
              "      <td>Retrieval</td>\n",
              "      <td>Relevant chunks ranked higher in retrieval</td>\n",
              "      <td>Higher âœ…</td>\n",
              "      <td>0.5909</td>\n",
              "      <td>0.6906</td>\n",
              "      <td>16.87%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Context Recall</td>\n",
              "      <td>Reference Answer â†’ Context</td>\n",
              "      <td>Retrieval</td>\n",
              "      <td>All needed information was retrieved</td>\n",
              "      <td>Higher âœ…</td>\n",
              "      <td>0.4722</td>\n",
              "      <td>0.7639</td>\n",
              "      <td>61.77%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Answer Relevancy</td>\n",
              "      <td>Question â†” Response</td>\n",
              "      <td>Generation</td>\n",
              "      <td>Answer directly addresses the question</td>\n",
              "      <td>Higher âœ…</td>\n",
              "      <td>0.7900</td>\n",
              "      <td>0.9555</td>\n",
              "      <td>20.95%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Faithfulness</td>\n",
              "      <td>Response â†’ Context</td>\n",
              "      <td>Generation</td>\n",
              "      <td>Answer claims supported by retrieved context</td>\n",
              "      <td>Higher âœ…</td>\n",
              "      <td>0.7648</td>\n",
              "      <td>0.8815</td>\n",
              "      <td>15.26%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Factual Correctness</td>\n",
              "      <td>Response â†” Reference Answer</td>\n",
              "      <td>End-to-End</td>\n",
              "      <td>Answer statements match ground truth</td>\n",
              "      <td>Higher âœ…</td>\n",
              "      <td>0.5755</td>\n",
              "      <td>0.5933</td>\n",
              "      <td>3.09%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Metric                   Comparison       Stage  \\\n",
              "0    Context Precision           Question â†’ Context   Retrieval   \n",
              "1       Context Recall   Reference Answer â†’ Context   Retrieval   \n",
              "2     Answer Relevancy          Question â†” Response  Generation   \n",
              "3         Faithfulness           Response â†’ Context  Generation   \n",
              "4  Factual Correctness  Response â†” Reference Answer  End-to-End   \n",
              "\n",
              "                               What it Measures    Better Baseline Score  \\\n",
              "0    Relevant chunks ranked higher in retrieval  Higher âœ…         0.5909   \n",
              "1          All needed information was retrieved  Higher âœ…         0.4722   \n",
              "2        Answer directly addresses the question  Higher âœ…         0.7900   \n",
              "3  Answer claims supported by retrieved context  Higher âœ…         0.7648   \n",
              "4          Answer statements match ground truth  Higher âœ…         0.5755   \n",
              "\n",
              "  Semantic Score Improvement  \n",
              "0         0.6906      16.87%  \n",
              "1         0.7639      61.77%  \n",
              "2         0.9555      20.95%  \n",
              "3         0.8815      15.26%  \n",
              "4         0.5933       3.09%  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create comprehensive comparison table with metadata\n",
        "comparison_data = {\n",
        "    'Metric': [\n",
        "        'Context Precision',\n",
        "        'Context Recall', \n",
        "        'Answer Relevancy',\n",
        "        'Faithfulness',\n",
        "        'Factual Correctness'\n",
        "    ],\n",
        "    'Comparison': [\n",
        "        'Question â†’ Context',\n",
        "        'Reference Answer â†’ Context',\n",
        "        'Question â†” Response',\n",
        "        'Response â†’ Context',\n",
        "        'Response â†” Reference Answer'\n",
        "    ],\n",
        "    'Stage': [\n",
        "        'Retrieval',\n",
        "        'Retrieval',\n",
        "        'Generation',\n",
        "        'Generation',\n",
        "        'End-to-End'\n",
        "    ],\n",
        "    'What it Measures': [\n",
        "        'Relevant chunks ranked higher in retrieval',\n",
        "        'All needed information was retrieved',\n",
        "        'Answer directly addresses the question',\n",
        "        'Answer claims supported by retrieved context',\n",
        "        'Answer statements match ground truth' ,\n",
        "    ],\n",
        "    'Better': [\n",
        "        'Higher âœ…',\n",
        "        'Higher âœ…',\n",
        "        'Higher âœ…',\n",
        "        'Higher âœ…',\n",
        "        'Higher âœ…'\n",
        "    ],\n",
        "    'Baseline Score': [\n",
        "        f\"{np.nanmean(baseline_scores['context_precision']):.4f}\",\n",
        "        f\"{np.nanmean(baseline_scores['context_recall']):.4f}\",\n",
        "        f\"{np.nanmean(baseline_scores['answer_relevancy']):.4f}\",\n",
        "        f\"{np.nanmean(baseline_scores['faithfulness']):.4f}\",\n",
        "        f\"{np.nanmean(baseline_scores['factual_correctness']):.4f}\"\n",
        "    ],\n",
        "    'Semantic Score': [\n",
        "        f\"{np.nanmean(semantic_scores['context_precision']):.4f}\",\n",
        "        f\"{np.nanmean(semantic_scores['context_recall']):.4f}\",\n",
        "        f\"{np.nanmean(semantic_scores['answer_relevancy']):.4f}\",\n",
        "        f\"{np.nanmean(semantic_scores['faithfulness']):.4f}\",\n",
        "        f\"{np.nanmean(semantic_scores['factual_correctness']):.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Calculate improvement\n",
        "comparison_df['Improvement'] = comparison_df.apply(\n",
        "    lambda row: f\"{((float(row['Semantic Score']) - float(row['Baseline Score'])) / float(row['Baseline Score']) * 100):.2f}%\"\n",
        "    if float(row['Baseline Score']) > 0 else \"N/A\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "comparison_df\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
